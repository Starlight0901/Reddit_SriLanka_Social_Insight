{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097bf05-6241-43f3-934c-0ca02218e8f6",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "672397e2-7d58-4142-a3b2-b5dd095428d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tharu\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: emoji in c:\\users\\tharu\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp313-cp313-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.11-cp313-cp313-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.2 MB 763.2 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.8/14.2 MB 967.5 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 1.0/14.2 MB 1.1 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.3/14.2 MB 1.1 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.3/14.2 MB 1.1 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.8/14.2 MB 1.1 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 2.1/14.2 MB 1.2 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 2.4/14.2 MB 1.2 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.6/14.2 MB 1.2 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.6/14.2 MB 1.2 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 3.1/14.2 MB 1.1 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 3.4/14.2 MB 1.2 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.7/14.2 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 4.2/14.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.5/14.2 MB 1.3 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 5.0/14.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.2/14.2 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.8/14.2 MB 1.4 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 6.3/14.2 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.8/14.2 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 7.3/14.2 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 7.6/14.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.1/14.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 8.7/14.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 9.2/14.2 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 9.7/14.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.0/14.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.5/14.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 11.0/14.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.3/14.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.3/14.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.8/14.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.3/14.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 12.6/14.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/14.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.1/14.2 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.6/14.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl (117 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl (653 kB)\n",
      "   ---------------------------------------- 0.0/653.1 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 524.3/653.1 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 653.1/653.1 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.3-cp313-cp313-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.2 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.2/6.2 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, cymem, cloudpathlib, catalogue, blis, typer-slim, srsly, preshed, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/16 [wasabi]\n",
      "   -- -------------------------------------  1/16 [spacy-loggers]\n",
      "   ----- ----------------------------------  2/16 [spacy-legacy]\n",
      "   ------- --------------------------------  3/16 [smart-open]\n",
      "   ------- --------------------------------  3/16 [smart-open]\n",
      "   ------------ ---------------------------  5/16 [cymem]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   ----------------- ----------------------  7/16 [catalogue]\n",
      "   -------------------- -------------------  8/16 [blis]\n",
      "   -------------------- -------------------  8/16 [blis]\n",
      "   -------------------- -------------------  8/16 [blis]\n",
      "   ---------------------- -----------------  9/16 [typer-slim]\n",
      "   ---------------------- -----------------  9/16 [typer-slim]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   --------------------------- ------------ 11/16 [preshed]\n",
      "   ------------------------------ --------- 12/16 [confection]\n",
      "   -------------------------------- ------- 13/16 [weasel]\n",
      "   -------------------------------- ------- 13/16 [weasel]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ---------------------------------------- 16/16 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "970ff0e2-6eae-4c89-a7f1-41e6072aa903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 2.9/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.5/12.8 MB 2.4 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e37ee78a-983f-40b7-8add-c947f48b6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\tharu\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.1 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 0.8/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd7c-fc07-4994-8853-bcad1c6ed0ac",
   "metadata": {},
   "source": [
    "# Commit to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b657452-1d16-4c01-9ed3-e38dfd5dc90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"Data text Preprocessing and cleaning\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260df94-6335-4131-b1c5-962708bb99d7",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "141bad06-47a7-4b41-bbfd-0e62b5c7de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from langdetect import detect\n",
    "import langdetect\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a289-471b-440f-8e73-57d9c0c0120a",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237f29ed-ddac-4873-b8f7-b54a183f4f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_14140\\3966237626.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source keyword       id   created_utc             author subreddit  \\\n",
       "0  post    new     NaN  1otaemb  1.762771e+09     Cookiehere6969  srilanka   \n",
       "1  post    new     NaN  1otaam5  1.762770e+09           oshan789  srilanka   \n",
       "2  post    new     NaN  1ot9w1v  1.762769e+09          mgssjjsks  srilanka   \n",
       "3  post    new     NaN  1ot9kwe  1.762768e+09  Critical_Rise_exe  srilanka   \n",
       "4  post    new     NaN  1ot9h2f  1.762767e+09       No-Leave8971  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                  Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "\n",
       "  parent_post  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc654212-751a-4707-ae63-580c3f5c958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70861, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset shape:\", df_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115c4822-97df-4205-a720-40df51d9dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates based on 'content':\n",
      "Empty DataFrame\n",
      "Columns: [type, source, keyword, id, created_utc, author, subreddit, content, score, num_comments, parent_post]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on a specific column\n",
    "duplicates_name = df_posts.duplicated(subset=['content'])\n",
    "print(\"\\nDuplicates based on 'content':\")\n",
    "print(df_posts[duplicates_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8c3f2e-a6a3-4abf-9a83-44d8c0527e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971e6205-4161-488b-95aa-fe05e2398424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc55767c-f6b5-4d1b-a33d-1d52e211ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword             0\n",
      "id                  0\n",
      "created_utc         0\n",
      "author              0\n",
      "subreddit           0\n",
      "content             0\n",
      "score               0\n",
      "num_comments    52020\n",
      "parent_post         0\n",
      "dtype: int64\n",
      "\n",
      " Dataset shape: (70860, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id   created_utc             author  \\\n",
       "0  post    new  no keyword  1otaemb  1.762771e+09     Cookiehere6969   \n",
       "1  post    new  no keyword  1otaam5  1.762770e+09           oshan789   \n",
       "2  post    new  no keyword  1ot9w1v  1.762769e+09          mgssjjsks   \n",
       "3  post    new  no keyword  1ot9kwe  1.762768e+09  Critical_Rise_exe   \n",
       "4  post    new  no keyword  1ot9h2f  1.762767e+09       No-Leave8971   \n",
       "\n",
       "  subreddit                                            content  score  \\\n",
       "0  srilanka  Is this a Scam or good investment? Haritha Lan...    2.0   \n",
       "1  srilanka  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0   \n",
       "2  srilanka  Whats your hot take on Sri Lanka as the title ...    3.0   \n",
       "3  srilanka                  Is the rs.11 deals real in Daraz?    1.0   \n",
       "4  srilanka  Need advice from the experts üôè [](https://www....    2.0   \n",
       "\n",
       "   num_comments parent_post  \n",
       "0           1.0     no post  \n",
       "1           0.0     no post  \n",
       "2           8.0     no post  \n",
       "3           3.0     no post  \n",
       "4           0.0     no post  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill 'keyword' missing values\n",
    "df_posts['keyword'] = df_posts['keyword'].fillna('no keyword')\n",
    "\n",
    "# Fill 'author' missing values\n",
    "df_posts['author'] = df_posts['author'].fillna('no author')\n",
    "\n",
    "# Fill 'score' missing values with the median\n",
    "median_score = df_posts['score'].median()\n",
    "df_posts['score'] = df_posts['score'].fillna(median_score)\n",
    "\n",
    "# Fill 'num_comments' missing values with 0 only where source == 'comments'\n",
    "mask = df_posts['source'] == 'comments'\n",
    "df_posts.loc[mask, 'num_comments'] = df_posts.loc[mask, 'num_comments'].fillna(0)\n",
    "\n",
    "# Fill 'parent_post' missing values\n",
    "df_posts['parent_post'] = df_posts['parent_post'].fillna('no post')\n",
    "\n",
    "# Drop rows where 'content' is missing (only 1 row)\n",
    "df_posts = df_posts.dropna(subset=['content'])\n",
    "# reset the index\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) \n",
    "\n",
    "print(\"\\n Dataset shape:\", df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9bf730-1b88-45fc-8826-c2191464a088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:40:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70855</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozebo7</td>\n",
       "      <td>Dhanagg</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>NEWSWIRE\\n\\nSri Lanka flags\\noutside Rawalpind...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:55:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70856</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozdn89</td>\n",
       "      <td>Unreal_realist-7381</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>DOT STUDIOS PRESENTS\\nPASAN DOMINIC HASALAKA T...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70857</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozdi55</td>\n",
       "      <td>wiknew1</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>sarasavi fi Q\\n\\nTHE BOOKSHOP\\nOL LIST SARASAV...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:07:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70858</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozbed2</td>\n",
       "      <td>smllcheeseburger</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Lamborghini Urus Twin turbo V8 2025\\n\\nPosted ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>08:54:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70859</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1oz8366</td>\n",
       "      <td>Crimson_roses154</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Lid ao ol on oe at jas\\nPTAA UO ¬©</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>05:28:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70860 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           type source     keyword       id               author subreddit  \\\n",
       "0          post    new  no keyword  1otaemb       Cookiehere6969  srilanka   \n",
       "1          post    new  no keyword  1otaam5             oshan789  srilanka   \n",
       "2          post    new  no keyword  1ot9w1v            mgssjjsks  srilanka   \n",
       "3          post    new  no keyword  1ot9kwe    Critical_Rise_exe  srilanka   \n",
       "4          post    new  no keyword  1ot9h2f         No-Leave8971  srilanka   \n",
       "...         ...    ...         ...      ...                  ...       ...   \n",
       "70855  img_post    url  no keyword  1ozebo7              Dhanagg  srilanka   \n",
       "70856  img_post    url  no keyword  1ozdn89  Unreal_realist-7381  srilanka   \n",
       "70857  img_post    url  no keyword  1ozdi55              wiknew1  srilanka   \n",
       "70858  img_post    url  no keyword  1ozbed2     smllcheeseburger  srilanka   \n",
       "70859  img_post    url  no keyword  1oz8366     Crimson_roses154  srilanka   \n",
       "\n",
       "                                                 content  score  num_comments  \\\n",
       "0      Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1      Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2      Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                      Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4      Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "...                                                  ...    ...           ...   \n",
       "70855  NEWSWIRE\\n\\nSri Lanka flags\\noutside Rawalpind...    3.0           NaN   \n",
       "70856  DOT STUDIOS PRESENTS\\nPASAN DOMINIC HASALAKA T...    3.0           NaN   \n",
       "70857  sarasavi fi Q\\n\\nTHE BOOKSHOP\\nOL LIST SARASAV...    3.0           NaN   \n",
       "70858  Lamborghini Urus Twin turbo V8 2025\\n\\nPosted ...    3.0           NaN   \n",
       "70859                  Lid ao ol on oe at jas\\nPTAA UO ¬©    3.0           NaN   \n",
       "\n",
       "      parent_post created_date created_time  \n",
       "0         no post   2025-11-10     10:33:16  \n",
       "1         no post   2025-11-10     10:26:02  \n",
       "2         no post   2025-11-10     10:00:29  \n",
       "3         no post   2025-11-10     09:40:57  \n",
       "4         no post   2025-11-10     09:33:57  \n",
       "...           ...          ...          ...  \n",
       "70855     no post   2025-11-17     11:55:19  \n",
       "70856     no post   2025-11-17     11:16:00  \n",
       "70857     no post   2025-11-17     11:07:48  \n",
       "70858     no post   2025-11-17     08:54:42  \n",
       "70859     no post   2025-11-17     05:28:02  \n",
       "\n",
       "[70860 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting date column into a readable format\n",
    "df_posts['created_date'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.date\n",
    "df_posts['created_time'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.time\n",
    "\n",
    "# Drop the original 'created_utc' column\n",
    "df_posts.drop(columns=['created_utc'], inplace=True)\n",
    "df_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4e95-d80e-45a3-a350-8c9920010d27",
   "metadata": {},
   "source": [
    "### Drop non-english data (sinhala and tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8f705a-fb07-4324-83f1-f3198e3ebb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.count of 0        en\n",
       "1        en\n",
       "2        en\n",
       "3        en\n",
       "4        en\n",
       "         ..\n",
       "70855    id\n",
       "70856    en\n",
       "70857    en\n",
       "70858    en\n",
       "70859    et\n",
       "Name: language, Length: 70860, dtype: object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Create a new column for language\n",
    "df_posts['language'] = df_posts['content'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f0387c0-c623-44f6-bd16-9e0c725d21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69773, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:40:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id             author subreddit  \\\n",
       "0  post    new  no keyword  1otaemb     Cookiehere6969  srilanka   \n",
       "1  post    new  no keyword  1otaam5           oshan789  srilanka   \n",
       "2  post    new  no keyword  1ot9w1v          mgssjjsks  srilanka   \n",
       "3  post    new  no keyword  1ot9kwe  Critical_Rise_exe  srilanka   \n",
       "4  post    new  no keyword  1ot9h2f       No-Leave8971  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                  Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "\n",
       "  parent_post created_date created_time  \n",
       "0     no post   2025-11-10     10:33:16  \n",
       "1     no post   2025-11-10     10:26:02  \n",
       "2     no post   2025-11-10     10:00:29  \n",
       "3     no post   2025-11-10     09:40:57  \n",
       "4     no post   2025-11-10     09:33:57  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only English posts\n",
    "df_posts = df_posts[df_posts['language'] == 'en'].copy()\n",
    "\n",
    "# Drop the language column\n",
    "df_posts.drop(columns=['language'], inplace=True)\n",
    "print(df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868e0ae-d838-4d46-a2ae-be17eca49662",
   "metadata": {},
   "source": [
    "### **1. Language Filtering**\n",
    "\n",
    "* [‚úîÔ∏è] Detect language of each post.\n",
    "* [‚úîÔ∏è] Remove posts in Sinhala, Tamil, or any non-English languages.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Text Normalization**\n",
    "\n",
    "* [ ] Convert all text to lowercase.\n",
    "* [ ] Strip leading and trailing whitespaces.\n",
    "* [ ] Normalize unicode characters (optional, e.g., accented letters).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Removing Irrelevant Content**\n",
    "\n",
    "* [ ] Remove URLs (e.g., `http://`, `https://`, `www.`).\n",
    "* [ ] Remove emojis and emoticons.\n",
    "* [ ] Remove platform-specific metadata (e.g., `reddit`, `u/username`, `r/subreddit`, `comments`).\n",
    "* [ ] Remove escape sequences (e.g., `\\n`, `\\t`).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Handling Special Characters**\n",
    "\n",
    "* [ ] Remove punctuation marks (`!`, `.`, `,`, `?`, etc.).\n",
    "* [ ] Remove other non-alphanumeric characters (`@`, `#`, `%`, `^`, etc.).\n",
    "* [ ] Optionally remove numbers (if not needed for analysis).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Tokenization and Text Structuring**\n",
    "\n",
    "* [ ] Tokenize text into words (if needed for further analysis).\n",
    "* [ ] Optionally remove stopwords (`the`, `is`, `and`, etc.).\n",
    "* [ ] Optionally lemmatize or stem words.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Filtering by Length**\n",
    "\n",
    "* [ ] Remove posts with fewer than 25 words.\n",
    "* [ ] Remove posts with more than 1000 words.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Corpus Analysis**\n",
    "\n",
    "* [ ] Calculate the total number of words in the cleaned dataset.\n",
    "* [ ] Calculate the number of unique words in the cleaned dataset.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Optional / Advanced Steps**\n",
    "\n",
    "* [‚úîÔ∏è] Handle duplicate posts if any.\n",
    "* [ ] Correct common typos or spelling errors.\n",
    "* [ ] Normalize spacing between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269d332-f121-47a5-826a-6d39a9fa12f5",
   "metadata": {},
   "source": [
    "# Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a65b95f-5a8c-4d3f-9bdf-7bda2e0014fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in cleaned corpus: 2000553\n",
      "Unique words in cleaned corpus: 61315\n"
     ]
    }
   ],
   "source": [
    "# Load English model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to replace emojis with textual description\n",
    "def emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # 1. Convert emojis to text\n",
    "    text = emoji_to_text(text)\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove Reddit platform metadata (u/username, r/subreddit)\n",
    "    text = re.sub(r'u\\/\\w+', '', text)\n",
    "    text = re.sub(r'r\\/\\w+', '', text)\n",
    "    \n",
    "    # 4. Remove escape sequences and extra whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 5. Remove punctuation (except within words like can't, won't)\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    \n",
    "    # 6. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 7. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 8. Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # 9. Lemmatization\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # 10. Join back into string\n",
    "    cleaned_text = \" \".join(lemmatized)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning function\n",
    "df_posts['content_cleaned'] = df_posts['content'].astype(str).apply(clean_text)\n",
    "\n",
    "# 11. Filter posts by word count\n",
    "min_words = 25\n",
    "max_words = 1000\n",
    "\n",
    "def word_count_filter(text):\n",
    "    count = len(text.split())\n",
    "    return min_words <= count <= max_words\n",
    "\n",
    "df_posts = df_posts[df_posts['content_cleaned'].apply(word_count_filter)]\n",
    "\n",
    "# 12. Report corpus statistics\n",
    "all_text = \" \".join(df_posts['content_cleaned'])\n",
    "all_words = all_text.split()\n",
    "total_words = len(all_words)\n",
    "unique_words = len(set(all_words))\n",
    "\n",
    "print(f\"Total words in cleaned corpus: {total_words}\")\n",
    "print(f\"Unique words in cleaned corpus: {unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed2c7e78-8876-4cb9-9dd6-3e1dccc7eaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "      <th>content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "      <td>scam good investment haritha lanka agarwood pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "      <td>villa unit sale unawatuna sri lanka new projec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "      <td>need advice expert folded_hand plan podcast fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9dyw</td>\n",
       "      <td>hotstar10</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Confusion Over Paddock Club Nugegoda‚Äôs Halal S...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:28:19</td>\n",
       "      <td>confusion paddock club nugegoda halal status o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9da2</td>\n",
       "      <td>prav_u</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Tour to Kanneliya Rain Forest I‚Äôm planning a g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:26:59</td>\n",
       "      <td>tour kanneliya rain forest I m plan group visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot8r8a</td>\n",
       "      <td>Unusual-Witness-7304</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>ChatGPT vs electricians, my house wiring is no...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>08:46:44</td>\n",
       "      <td>chatgpt vs electricians house wiring science e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot8lir</td>\n",
       "      <td>negative-impactr8888</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>SLTMobitel changed the superuser router passwo...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>08:36:16</td>\n",
       "      <td>sltmobitel change superuser router password tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot8l95</td>\n",
       "      <td>Jakesbond007</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Can we hand carry medicine from abroad into Sr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>08:35:47</td>\n",
       "      <td>hand carry medicine abroad sri lanka hello med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot8kjb</td>\n",
       "      <td>sorenxv</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>How Do I Get A Letter From Church? My bestfrie...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>08:34:33</td>\n",
       "      <td>get letter church bestfriend 's newborn gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot87vn</td>\n",
       "      <td>ScreenshotSmuggler</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Guess where this video is from. Let's see how ...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>08:11:13</td>\n",
       "      <td>guess video let us see far you all get provinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot7k8g</td>\n",
       "      <td>PersonalityWeird4725</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Best Digital Marketing Agencies in Sri Lanka I...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>07:29:02</td>\n",
       "      <td>good digital marketing agency sri lanka india ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot7jy8</td>\n",
       "      <td>John-Travel-8490</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Traveling around Sri Lanka with two checked ba...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>07:28:32</td>\n",
       "      <td>travel around sri lanka two check bag hello gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot7inn</td>\n",
       "      <td>Emotional_Zebra2006</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need a specific info regarding a car accident ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>07:26:13</td>\n",
       "      <td>need specific info regard car accident corolla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot7gt5</td>\n",
       "      <td>WarEast4764</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Ella hotel with jaw-dropping views + quiet We‚Äô...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>07:22:57</td>\n",
       "      <td>ella hotel jawdropping view quiet head ella wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot7c8u</td>\n",
       "      <td>Fantastic-Entry-2492</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Clarification about Artificial intelligence De...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>07:14:52</td>\n",
       "      <td>clarification artificial intelligence degree i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot7b6s</td>\n",
       "      <td>Fantastic-Entry-2492</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>About Credit Cards in Sri Lanka Hi Guys, \\n\\nC...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>07:12:58</td>\n",
       "      <td>credit card sri lanka hi guy currently work so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot71ir</td>\n",
       "      <td>Technical-Truth-2073</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is Tech the only industry offering good pay in...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>06:55:58</td>\n",
       "      <td>tech industry offer good pay sl I ve notice te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot70mb</td>\n",
       "      <td>Frosty_Low5938</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Vat over cess is this a good decision by the G...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>06:54:26</td>\n",
       "      <td>vat cess good decision government government g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot5wkr</td>\n",
       "      <td>StarlightMisery13</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the bulath (betel) leaf a drug itself? I ha...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>05:47:28</td>\n",
       "      <td>bulath betel leaf drug see many people old gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot4xaf</td>\n",
       "      <td>Least-Swordfish-4472</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>BIT External Degree UoC or UoM planning to enr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>04:52:19</td>\n",
       "      <td>bit external degree uoc uom planning enroll bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type source     keyword       id                author subreddit  \\\n",
       "0   post    new  no keyword  1otaemb        Cookiehere6969  srilanka   \n",
       "1   post    new  no keyword  1otaam5              oshan789  srilanka   \n",
       "4   post    new  no keyword  1ot9h2f          No-Leave8971  srilanka   \n",
       "5   post    new  no keyword  1ot9dyw             hotstar10  srilanka   \n",
       "6   post    new  no keyword  1ot9da2                prav_u  srilanka   \n",
       "7   post    new  no keyword  1ot8r8a  Unusual-Witness-7304  srilanka   \n",
       "8   post    new  no keyword  1ot8lir  negative-impactr8888  srilanka   \n",
       "9   post    new  no keyword  1ot8l95          Jakesbond007  srilanka   \n",
       "10  post    new  no keyword  1ot8kjb               sorenxv  srilanka   \n",
       "11  post    new  no keyword  1ot87vn    ScreenshotSmuggler  srilanka   \n",
       "14  post    new  no keyword  1ot7k8g  PersonalityWeird4725  srilanka   \n",
       "15  post    new  no keyword  1ot7jy8      John-Travel-8490  srilanka   \n",
       "16  post    new  no keyword  1ot7inn   Emotional_Zebra2006  srilanka   \n",
       "17  post    new  no keyword  1ot7gt5           WarEast4764  srilanka   \n",
       "19  post    new  no keyword  1ot7c8u  Fantastic-Entry-2492  srilanka   \n",
       "20  post    new  no keyword  1ot7b6s  Fantastic-Entry-2492  srilanka   \n",
       "21  post    new  no keyword  1ot71ir  Technical-Truth-2073  srilanka   \n",
       "22  post    new  no keyword  1ot70mb        Frosty_Low5938  srilanka   \n",
       "25  post    new  no keyword  1ot5wkr     StarlightMisery13  srilanka   \n",
       "28  post    new  no keyword  1ot4xaf  Least-Swordfish-4472  srilanka   \n",
       "\n",
       "                                              content  score  num_comments  \\\n",
       "0   Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1   Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "4   Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "5   Confusion Over Paddock Club Nugegoda‚Äôs Halal S...    0.0           4.0   \n",
       "6   Tour to Kanneliya Rain Forest I‚Äôm planning a g...    1.0           1.0   \n",
       "7   ChatGPT vs electricians, my house wiring is no...    6.0           5.0   \n",
       "8   SLTMobitel changed the superuser router passwo...    3.0           0.0   \n",
       "9   Can we hand carry medicine from abroad into Sr...    1.0           3.0   \n",
       "10  How Do I Get A Letter From Church? My bestfrie...    2.0           2.0   \n",
       "11  Guess where this video is from. Let's see how ...   21.0          43.0   \n",
       "14  Best Digital Marketing Agencies in Sri Lanka I...    1.0           5.0   \n",
       "15  Traveling around Sri Lanka with two checked ba...    1.0           1.0   \n",
       "16  Need a specific info regarding a car accident ...    1.0           3.0   \n",
       "17  Ella hotel with jaw-dropping views + quiet We‚Äô...    0.0           1.0   \n",
       "19  Clarification about Artificial intelligence De...    2.0           0.0   \n",
       "20  About Credit Cards in Sri Lanka Hi Guys, \\n\\nC...    4.0           8.0   \n",
       "21  Is Tech the only industry offering good pay in...   12.0          10.0   \n",
       "22  Vat over cess is this a good decision by the G...    2.0           0.0   \n",
       "25  Is the bulath (betel) leaf a drug itself? I ha...    2.0           2.0   \n",
       "28  BIT External Degree UoC or UoM planning to enr...    1.0           1.0   \n",
       "\n",
       "   parent_post created_date created_time  \\\n",
       "0      no post   2025-11-10     10:33:16   \n",
       "1      no post   2025-11-10     10:26:02   \n",
       "4      no post   2025-11-10     09:33:57   \n",
       "5      no post   2025-11-10     09:28:19   \n",
       "6      no post   2025-11-10     09:26:59   \n",
       "7      no post   2025-11-10     08:46:44   \n",
       "8      no post   2025-11-10     08:36:16   \n",
       "9      no post   2025-11-10     08:35:47   \n",
       "10     no post   2025-11-10     08:34:33   \n",
       "11     no post   2025-11-10     08:11:13   \n",
       "14     no post   2025-11-10     07:29:02   \n",
       "15     no post   2025-11-10     07:28:32   \n",
       "16     no post   2025-11-10     07:26:13   \n",
       "17     no post   2025-11-10     07:22:57   \n",
       "19     no post   2025-11-10     07:14:52   \n",
       "20     no post   2025-11-10     07:12:58   \n",
       "21     no post   2025-11-10     06:55:58   \n",
       "22     no post   2025-11-10     06:54:26   \n",
       "25     no post   2025-11-10     05:47:28   \n",
       "28     no post   2025-11-10     04:52:19   \n",
       "\n",
       "                                      content_cleaned  \n",
       "0   scam good investment haritha lanka agarwood pl...  \n",
       "1   villa unit sale unawatuna sri lanka new projec...  \n",
       "4   need advice expert folded_hand plan podcast fr...  \n",
       "5   confusion paddock club nugegoda halal status o...  \n",
       "6   tour kanneliya rain forest I m plan group visi...  \n",
       "7   chatgpt vs electricians house wiring science e...  \n",
       "8   sltmobitel change superuser router password tr...  \n",
       "9   hand carry medicine abroad sri lanka hello med...  \n",
       "10  get letter church bestfriend 's newborn gettin...  \n",
       "11  guess video let us see far you all get provinc...  \n",
       "14  good digital marketing agency sri lanka india ...  \n",
       "15  travel around sri lanka two check bag hello gu...  \n",
       "16  need specific info regard car accident corolla...  \n",
       "17  ella hotel jawdropping view quiet head ella wa...  \n",
       "19  clarification artificial intelligence degree i...  \n",
       "20  credit card sri lanka hi guy currently work so...  \n",
       "21  tech industry offer good pay sl I ve notice te...  \n",
       "22  vat cess good decision government government g...  \n",
       "25  bulath betel leaf drug see many people old gen...  \n",
       "28  bit external degree uoc uom planning enroll bi...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f78c3d-87c6-4462-8c99-5672afa68212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "773ebeac-28b9-4c21-a126-4420cd12afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caf√© ‚Äî full-width TEXT\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode characters using NFKC.\n",
    "    Converts full-width characters, combined characters, and compatibility characters \n",
    "    into a consistent canonical form.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"caf√© ‚Äî ÔΩÜÔΩïÔΩåÔΩåÔºçÔΩóÔΩâÔΩÑÔΩîÔΩà Ôº¥Ôº•Ôº∏Ôº¥\"\n",
    "normalized_text = normalize_unicode(raw_text)\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608468e1-0ae2-478b-9e8c-06ed618002e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539858eb-f3a0-428b-bcc6-5605f4376337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "# -----------------------------\n",
    "# 2.1 Word-level tokenizer + lemmatization + stopwords removal\n",
    "# -----------------------------\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# -----------------------------\n",
    "# 2.2 Prepare corpus for SentencePiece\n",
    "# -----------------------------\n",
    "corpus_file = \"corpus.txt\"\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in df_posts[\"content_cleaned\"]:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2.3 Train BPE tokenizer\n",
    "# -----------------------------\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=bpe --vocab_size=8000 --model_type=bpe\"\n",
    ")\n",
    "bpe = spm.SentencePieceProcessor()\n",
    "bpe.load(\"bpe.model\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2.4 Train Unigram LM tokenizer\n",
    "# -----------------------------\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=unigram --vocab_size=8000 --model_type=unigram\"\n",
    ")\n",
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(\"unigram.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efc7d6-2799-49b2-aa41-f98133f45dc1",
   "metadata": {},
   "source": [
    "# PHASE C ‚Äî Compare Tokenization Impact on Corpus Statistics\n",
    "\n",
    "We calculate:\n",
    "\n",
    "vocabulary size\n",
    "\n",
    "average tokens per document\n",
    "\n",
    "total tokens\n",
    "\n",
    "rare token frequency (<3 occurrences)\n",
    "\n",
    "OOV rate (for word-based tokenizers only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fc559e0-ef35-44b1-a305-7b918ad63139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORPUS STATISTICS PER TOKENIZER\n",
    "\n",
    "def compute_stats(tokenizer_fn, tokenizer_name):\n",
    "    all_tokens = []\n",
    "    doc_lengths = []\n",
    "\n",
    "    for text in df_posts[\"content_cleaned\"]:\n",
    "        tokens = tokenizer_fn(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        doc_lengths.append(len(tokens))\n",
    "\n",
    "    vocab = set(all_tokens)\n",
    "    counter = collections.Counter(all_tokens)\n",
    "\n",
    "    return {\n",
    "        \"Tokenizer\": tokenizer_name,\n",
    "        \"Vocabulary Size\": len(vocab),\n",
    "        \"Total Tokens\": len(all_tokens),\n",
    "        \"Avg Tokens per Doc\": sum(doc_lengths)/len(doc_lengths),\n",
    "        \"Rare Tokens (<3)\": sum(1 for t,c in counter.items() if c < 3)\n",
    "    }\n",
    "\n",
    "stats_word = compute_stats(word_tokenizer, \"Word+Lemmatization\")\n",
    "stats_bpe = compute_stats(lambda t: bpe.encode(t, out_type=str), \"BPE\")\n",
    "stats_unigram = compute_stats(lambda t: unigram.encode(t, out_type=str), \"Unigram LM\")\n",
    "\n",
    "stats_df = pd.DataFrame([stats_word, stats_bpe, stats_unigram])\n",
    "print(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414332c-d7e4-46fd-9940-014f0662f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM LANGUAGE MODEL\n",
    "\n",
    "# -----------------------------\n",
    "# 4.1 Dataset class\n",
    "# -----------------------------\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, token_ids, seq_len=30):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_len])\n",
    "        y = torch.tensor(self.data[idx+1:idx+1+self.seq_len])\n",
    "        return x, y\n",
    "\n",
    "# -----------------------------\n",
    "# 4.2 LSTM LM model\n",
    "# -----------------------------\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        o, _ = self.lstm(e)\n",
    "        return self.fc(o)\n",
    "\n",
    "# -----------------------------\n",
    "# 4.3 Training function\n",
    "# -----------------------------\n",
    "def train_lm(model, dataloader, epochs=2):\n",
    "    model.train()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optim.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# 4.4 Perplexity\n",
    "# -----------------------------\n",
    "def perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_loss, total_tokens = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += y.numel()\n",
    "\n",
    "    return math.exp(total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe1b9-d201-42ec-955c-04b0b4e6a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN LSTMs PER TOKENIZER\n",
    "\n",
    "SEQ, BATCH = 30, 32\n",
    "\n",
    "# -------- Word-level ----------\n",
    "word_vocab = {w:i for i,w in enumerate(set(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer)\n",
    ")))}\n",
    "word_id = lambda toks: [word_vocab[t] for t in toks if t in word_vocab]\n",
    "\n",
    "word_ids = list(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer).apply(word_id)\n",
    "))\n",
    "word_dl = DataLoader(LMDataset(word_ids, SEQ), batch_size=BATCH)\n",
    "lm_word = LSTMLM(len(word_vocab))\n",
    "lm_word = train_lm(lm_word, word_dl)\n",
    "pp_word = perplexity(lm_word, word_dl)\n",
    "\n",
    "# -------- BPE ----------\n",
    "bpe_ids = [i for t in df_posts[\"content_cleaned\"] for i in bpe.encode(t)]\n",
    "bpe_dl = DataLoader(LMDataset(bpe_ids, SEQ), batch_size=BATCH)\n",
    "lm_bpe = LSTMLM(bpe.get_piece_size())\n",
    "lm_bpe = train_lm(lm_bpe, bpe_dl)\n",
    "pp_bpe = perplexity(lm_bpe, bpe_dl)\n",
    "\n",
    "# -------- Unigram ----------\n",
    "uni_ids = [i for t in df_posts[\"content_cleaned\"] for i in unigram.encode(t)]\n",
    "uni_dl = DataLoader(LMDataset(uni_ids, SEQ), batch_size=BATCH)\n",
    "lm_uni = LSTMLM(unigram.get_piece_size())\n",
    "lm_uni = train_lm(lm_uni, uni_dl)\n",
    "pp_uni = perplexity(lm_uni, uni_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ab0b5-2234-455d-80e7-a9463431edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL COMPARISON TABLE\n",
    "\n",
    "final_results = pd.DataFrame({\n",
    "    \"Tokenizer\": [\"Word+Lemmatization\", \"BPE\", \"Unigram\"],\n",
    "    \"Vocabulary Size\": [stats_word[\"Vocabulary Size\"], stats_bpe[\"Vocabulary Size\"], stats_unigram[\"Vocabulary Size\"]],\n",
    "    \"Total Tokens\": [stats_word[\"Total Tokens\"], stats_bpe[\"Total Tokens\"], stats_unigram[\"Total Tokens\"]],\n",
    "    \"Avg Tokens per Doc\": [stats_word[\"Avg Tokens per Doc\"], stats_bpe[\"Avg Tokens per Doc\"], stats_unigram[\"Avg Tokens per Doc\"]],\n",
    "    \"Rare Tokens (<3)\": [stats_word[\"Rare Tokens (<3)\"], stats_bpe[\"Rare Tokens (<3)\"], stats_unigram[\"Rare Tokens (<3)\"]],\n",
    "    \"Perplexity\": [pp_word, pp_bpe, pp_uni]\n",
    "})\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ca153-cd81-4063-b7da-d8c76a47f659",
   "metadata": {},
   "source": [
    "## ‚úÖ PIPELINE EXPLANATION\n",
    "\n",
    "Phase 1 ‚Äî Cleaning\n",
    "\n",
    "Remove non-English posts\n",
    "\n",
    "Normalize unicode\n",
    "\n",
    "Remove URLs, emojis ‚Üí text\n",
    "\n",
    "Remove Reddit metadata, punctuation, stopwords\n",
    "\n",
    "Lowercase & strip\n",
    "\n",
    "Filter posts <25 or >1000 words\n",
    "\n",
    "Phase 2 ‚Äî Tokenization\n",
    "\n",
    "Word-level + lemmatization\n",
    "\n",
    "SentencePiece BPE\n",
    "\n",
    "SentencePiece Unigram\n",
    "\n",
    "Phase 3 ‚Äî Corpus Statistics\n",
    "\n",
    "Compute vocab size, total tokens, avg tokens/doc, rare tokens\n",
    "\n",
    "Phase 4 ‚Äî LSTM LM\n",
    "\n",
    "Build LSTM language model\n",
    "\n",
    "Train on each tokenization scheme\n",
    "\n",
    "Phase 5 ‚Äî Train LSTM + Perplexity\n",
    "\n",
    "Compare model performance\n",
    "\n",
    "Phase 6 ‚Äî Final Table\n",
    "\n",
    "Compare vocab, tokens, and perplexity\n",
    "\n",
    "Recommend best tokenizer (likely BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c44f5-69ed-4571-bb03-e152da3ffc92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
