{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1418de11-3664-498a-8b1a-54ab6658d68e",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af962f65-a6cc-437b-8fff-c4a86356b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8d026-8459-4780-ae10-d6b08330bae1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ed96a9d-31b1-4bdc-97b9-12cc1fc1603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a556f7-e7a5-4692-8b43-e7811d817d96",
   "metadata": {},
   "source": [
    "# Commit to Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981a4536-400d-4718-9fda-1f285e8f8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Categorizing posts for classification.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Data Preprocessing and Feature Engineering.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main f88de0c] Creating vector representations\n",
      " 8 files changed, 119164 insertions(+), 110589 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/Starlight0901/Reddit_SriLanka_Social_Insight.git\n",
      "   fef4748..f88de0c  main -> main\n"
     ]
    }
   ],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"Creating vector representations\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20b571-6324-45aa-9c5a-7b66ac2ca86e",
   "metadata": {},
   "source": [
    "# Load the saved tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8011b21d-6bd4-4395-a503-7021998213b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 0, 1835, 30, 2700, 905, 5, 3671, 71]\n",
      " ‚Åá his is a sample post\n"
     ]
    }
   ],
   "source": [
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(r\"C:\\Users\\tharu\\Documents\\GitHub\\Reddit_SriLanka_Social_Insight\\unigram.model\")  # specify the correct path\n",
    "\n",
    "# Encode text to token IDs\n",
    "token_ids = unigram.encode(\"This is a sample post\", out_type=int)\n",
    "\n",
    "# Decode back to text\n",
    "text = unigram.decode(token_ids)\n",
    "\n",
    "print(token_ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb647ea5-961d-4edf-bc82-c5a8701472a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "      <td>scam good investment haritha lanka agarwood pl...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:00:29</td>\n",
       "      <td>s hot sri lanka title say gimme hot</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "      <td>need advice expert plan podcast friend short e...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9dyw</td>\n",
       "      <td>hotstar10</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Confusion Over Paddock Club Nugegoda‚Äôs Halal S...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:28:19</td>\n",
       "      <td>confusion paddock club nugegoda s halal status...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9da2</td>\n",
       "      <td>prav_u</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Tour to Kanneliya Rain Forest I‚Äôm planning a g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:26:59</td>\n",
       "      <td>tour kanneliya rain forest m plan group visit ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id          author subreddit  \\\n",
       "0  post    new  no keyword  1otaemb  Cookiehere6969  srilanka   \n",
       "1  post    new  no keyword  1ot9w1v       mgssjjsks  srilanka   \n",
       "2  post    new  no keyword  1ot9h2f    No-Leave8971  srilanka   \n",
       "3  post    new  no keyword  1ot9dyw       hotstar10  srilanka   \n",
       "4  post    new  no keyword  1ot9da2          prav_u  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "2  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "3  Confusion Over Paddock Club Nugegoda‚Äôs Halal S...    0.0           4.0   \n",
       "4  Tour to Kanneliya Rain Forest I‚Äôm planning a g...    1.0           1.0   \n",
       "\n",
       "  parent_post created_date created_time  \\\n",
       "0     no post   2025-11-10     10:33:16   \n",
       "1     no post   2025-11-10     10:00:29   \n",
       "2     no post   2025-11-10     09:33:57   \n",
       "3     no post   2025-11-10     09:28:19   \n",
       "4     no post   2025-11-10     09:26:59   \n",
       "\n",
       "                                     content_cleaned  word_count  \n",
       "0  scam good investment haritha lanka agarwood pl...          32  \n",
       "1                s hot sri lanka title say gimme hot           8  \n",
       "2  need advice expert plan podcast friend short e...          57  \n",
       "3  confusion paddock club nugegoda s halal status...          41  \n",
       "4  tour kanneliya rain forest m plan group visit ...          32  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"cleaned_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b0e3c44-f496-43d1-b091-8c52cd53d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text to token IDs using the loaded tokenizer\n",
    "df_posts['tokens'] = df_posts['content_cleaned'].apply(lambda x: unigram.encode(x, out_type=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98cca213-7b11-4775-a4bb-511677ea0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_posts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f26d3f-1d76-42fb-937a-4d724504819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count shape: (48028, 172884)\n",
      "TF-IDF shape: (48028, 172884)\n",
      "LSA shape: (48028, 100)\n",
      "\n",
      "Training Word2Vec model...\n",
      "Word2Vec shape: (48028, 300)\n",
      "\n",
      "All vector representations generated successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Sparse Representations\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)\n",
    "X_count = count_vect.fit_transform(df['content_cleaned'])\n",
    "print(\"Count shape:\", X_count.shape)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)\n",
    "X_tfidf = tfidf_vect.fit_transform(df['content_cleaned'])\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n",
    "# -----------------------------\n",
    "#  Dense: LSA (Truncated SVD)\n",
    "# -----------------------------\n",
    "\n",
    "svd_components = 100\n",
    "svd = TruncatedSVD(n_components=svd_components, random_state=42)\n",
    "X_lsa = svd.fit_transform(X_tfidf)\n",
    "print(\"LSA shape:\", X_lsa.shape)\n",
    "\n",
    "# -----------------------------\n",
    "#  Dense: Word2Vec Embeddings\n",
    "# -----------------------------\n",
    "print(\"\\nTraining Word2Vec model...\")\n",
    "\n",
    "# 1. Tokenize text\n",
    "documents = df['content_cleaned'].astype(str).apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Train Word2Vec\n",
    "w2v_dim = 300  # embedding dimension\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=documents,\n",
    "    vector_size=w2v_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 3. Build document embeddings\n",
    "def doc_vector(words):\n",
    "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(w2v_dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_w2v = np.array([doc_vector(doc) for doc in documents])\n",
    "print(\"Word2Vec shape:\", X_w2v.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Save everything\n",
    "# -----------------------------\n",
    "joblib.dump(count_vect, 'count_vect.joblib')\n",
    "joblib.dump(tfidf_vect, 'tfidf_vect.joblib')\n",
    "joblib.dump(svd, 'svd_lsa.joblib')\n",
    "w2v_model.save(\"word2vec.model\")\n",
    "\n",
    "print(\"\\nAll vector representations generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ee97e-18dc-485a-85a0-ecdf6de9c9c0",
   "metadata": {},
   "source": [
    "## **Justification of Vector Representations**\n",
    "\n",
    "### **1. Sparse Representations**\n",
    "\n",
    "**a) Count Vector (Bag-of-Words)**\n",
    "\n",
    "* Choice: CountVectorizer with unigrams and bigrams.\n",
    "* Reason:\n",
    "  * Captures raw frequency of words/phrases in each document.\n",
    "  * Bigrams help detect short phrases and context (**‚Äúnot good‚Äù**, **‚Äúhigh risk‚Äù**).\n",
    "  * Sparse format is memory-efficient for high-dimensional data.\n",
    "* **Dimension:** `(48028, 172884)` ‚Üí 48k posts √ó 172k vocabulary features.\n",
    "\n",
    "**b) TF-IDF Vector**\n",
    "\n",
    "* **Choice:** TfidfVectorizer with unigrams and bigrams.\n",
    "* **Reason:**\n",
    "\n",
    "  * Improves on raw counts by down-weighting very common words and up-weighting discriminative terms.\n",
    "  * Reduces noise and highlights informative features for clustering/classification.\n",
    "* **Dimension:** `(48028, 172884)` ‚Üí same vocabulary, different feature weighting.\n",
    "\n",
    "### **2. Dense Representations**\n",
    "\n",
    "**a) LSA (Truncated SVD)**\n",
    "\n",
    "* **Choice:** Reduce TF-IDF matrix to 100 latent dimensions.\n",
    "* **Reason:**\n",
    "\n",
    "  * Captures **latent semantic structure** rather than raw word counts.\n",
    "  * Reduces dimensionality from 172k ‚Üí 100, making downstream models faster and less prone to overfitting.\n",
    "  * Dense vectors allow similarity-based clustering (e.g., KMeans) to work better.\n",
    "* **Dimension:** `(48028, 100)` ‚Üí 48k documents √ó 100 latent features.\n",
    "\n",
    "**b) Word2Vec (Average Word Embeddings)**\n",
    "\n",
    "* **Choice:** Train 300-dimensional word embeddings and average per document.\n",
    "* **Reason:**\n",
    "\n",
    "  * Captures **semantic meaning** of words and documents, not just frequency.\n",
    "  * Dense, compact, and suitable for clustering or classification.\n",
    "  * Complementary to LSA because it uses **contextual similarity** rather than linear algebra on term-frequency.\n",
    "* **Dimension:** `(48028, 300)` ‚Üí 48k documents √ó 300-dimensional dense vectors.\n",
    "\n",
    "\n",
    " **Summary Table**\n",
    "\n",
    "| Representation | Type   | Dimensionality  | Justification                                                                    |\n",
    "| -------------- | ------ | --------------- | -------------------------------------------------------------------------------- |\n",
    "| Count          | Sparse | (48028, 172884) | Captures raw term frequency with unigrams + bigrams; good baseline for ML        |\n",
    "| TF-IDF         | Sparse | (48028, 172884) | Highlights informative terms, reduces noise from frequent words                  |\n",
    "| LSA            | Dense  | (48028, 100)    | Reduces dimensionality, captures latent semantic structure for better clustering |\n",
    "| Word2Vec       | Dense  | (48028, 300)    | Captures semantic meaning; dense embeddings improve similarity-based tasks       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a72bb-8c9b-4e50-977c-36215b24ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dcad65-40be-437a-a875-4174abe9e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac80c8-1c99-45c4-afb1-304cd149ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c842e-8ba9-496b-a86e-468914c1d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766ec34-1d8f-48ed-9999-0c82beb292db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dea73-8e72-4e33-a9b4-2e44110ab5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dac78-843a-4f35-84d6-e6de0ca75f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7efcd-e74b-40b1-9e5c-cd161208d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907332d2-5fc0-462a-87cb-259bf06c3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d285ada-b58d-48a7-ad3c-b1e1a16e7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Use Word2Vec\n",
    "X_input = X_w2v \n",
    "\n",
    "# Try different k values\n",
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_input)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_input, kmeans.labels_))\n",
    "\n",
    "# Plot SSE (Elbow) and Silhouette Score\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (SSE)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b44ad57-938f-4ab2-a773-af70443ca320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster distribution:\n",
      "{np.int32(0): np.int64(6214), np.int32(1): np.int64(3039), np.int32(2): np.int64(5893), np.int32(3): np.int64(2124), np.int32(4): np.int64(5278), np.int32(5): np.int64(25480)}\n"
     ]
    }
   ],
   "source": [
    "best_k = 6  # example; choose based on Elbow/Silhouette\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_input)\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fa2e7e8-2389-47e3-b1a8-5c24a2109262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62510ec9-33ea-49f8-b5a6-9d033886067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=best_k, random_state=42, learning_method='batch', max_iter=20)\n",
    "X_lda = lda.fit_transform(X_tfidf)  # document-topic distribution\n",
    "\n",
    "# Assign dominant topic as category\n",
    "lda_labels = X_lda.argmax(axis=1)\n",
    "df['lda_category'] = lda_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3aa25a55-abeb-4726-9bb2-297b25802209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: people, sri, like, country, lanka, sri lanka, good, think, know, government\n",
      "Topic 1: like, sri, good, people, package, know, slt, lanka, sri lanka, dialog\n",
      "Topic 2: sri, lanka, sri lanka, good, travel, colombo, like, day, visit, know\n",
      "Topic 3: people, like, sri, good, know, lanka, sri lanka, think, time, go\n",
      "Topic 4: degree, job, like, work, sri, good, university, year, lanka, sri lanka\n",
      "Topic 5: sri, like, people, lanka, sri lanka, think, good, know, pay, country\n"
     ]
    }
   ],
   "source": [
    "def print_top_terms(model, feature_names, n_top_words=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[-n_top_words:][::-1]]\n",
    "        print(f\"Topic {idx}: {', '.join(top_features)}\")\n",
    "\n",
    "tfidf_feature_names = tfidf_vect.get_feature_names_out()\n",
    "print_top_terms(lda, tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a678ef6-3e83-4eae-bf40-09d3dde669ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
