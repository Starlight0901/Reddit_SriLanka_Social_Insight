{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbdb06e-fccf-4592-8e96-13ebc0e5558e",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4c7ac1-bef9-4f17-92d5-422baad94f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581753f6-4399-400b-914e-e35c307ac573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc4f5ef-ec1a-420f-8a14-4218229b31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tharu\\anaconda3\\envs\\GPU-Env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f75569-a268-4705-b956-8965e77f76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spherecluster in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spherecluster) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spherecluster) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spherecluster) (1.7.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spherecluster) (9.0.1)\n",
      "Requirement already satisfied: nose in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spherecluster) (1.3.7)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn>=0.20->spherecluster) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn>=0.20->spherecluster) (3.6.0)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pytest->spherecluster) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pytest->spherecluster) (1.3.0)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pytest->spherecluster) (2.3.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pytest->spherecluster) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pytest->spherecluster) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pytest->spherecluster) (2.19.2)\n",
      "Requirement already satisfied: tomli>=1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pytest->spherecluster) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from exceptiongroup>=1->pytest->spherecluster) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spherecluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff80d7-2b58-412b-a22a-7f0b680cf1f1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0354fc3b-6cb8-47f7-8e22-5407f6d9857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82166ec-a2ea-4738-9ffd-76063adba893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ce617-4971-4e3e-b28d-6e317ade8636",
   "metadata": {},
   "source": [
    "# Commit to Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f97c2e-52ad-4a93-95f7-5c6b1327ce43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of '.ipynb_checkpoints/Categorizing posts for classification-Copy1-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Categorizing posts for classification-Copy1.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of '.ipynb_checkpoints/Untitled-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Untitled.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 9e8f3d8] Clustering and classification - redo\n",
      " 5 files changed, 1823 insertions(+), 878 deletions(-)\n",
      " create mode 100644 .ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
      " create mode 100644 Untitled.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/Starlight0901/Reddit_SriLanka_Social_Insight.git\n",
      "   71f0f5f..9e8f3d8  main -> main\n"
     ]
    }
   ],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"Clustering and classification - redo\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b5664-b905-4216-a9b2-9be5f0d33401",
   "metadata": {},
   "source": [
    "# Load the saved tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc6609e-19ff-4561-ac9f-7d93e10f4b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(r\"C:\\Users\\tharu\\Documents\\GitHub\\Reddit_SriLanka_Social_Insight\\unigram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030935bb-d525-4596-88a0-ca11d1191585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "      <td>scam good investment haritha lanka agarwood pl...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "      <td>villa unit sale unawatuna sri lanka new projec...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "      <td>need advice expert folded_hands plan podcast f...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9dyw</td>\n",
       "      <td>hotstar10</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Confusion Over Paddock Club Nugegoda‚Äôs Halal S...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:28:19</td>\n",
       "      <td>confusion paddock club nugegoda halal status o...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9da2</td>\n",
       "      <td>prav_u</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Tour to Kanneliya Rain Forest I‚Äôm planning a g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:26:59</td>\n",
       "      <td>tour kanneliya rain forest plan group visit ka...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id          author subreddit  \\\n",
       "0  post    new  no keyword  1otaemb  Cookiehere6969  srilanka   \n",
       "1  post    new  no keyword  1otaam5        oshan789  srilanka   \n",
       "2  post    new  no keyword  1ot9h2f    No-Leave8971  srilanka   \n",
       "3  post    new  no keyword  1ot9dyw       hotstar10  srilanka   \n",
       "4  post    new  no keyword  1ot9da2          prav_u  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "3  Confusion Over Paddock Club Nugegoda‚Äôs Halal S...    0.0           4.0   \n",
       "4  Tour to Kanneliya Rain Forest I‚Äôm planning a g...    1.0           1.0   \n",
       "\n",
       "  parent_post created_date created_time  \\\n",
       "0     no post   2025-11-10     10:33:16   \n",
       "1     no post   2025-11-10     10:26:02   \n",
       "2     no post   2025-11-10     09:33:57   \n",
       "3     no post   2025-11-10     09:28:19   \n",
       "4     no post   2025-11-10     09:26:59   \n",
       "\n",
       "                                     content_cleaned  word_count  \n",
       "0  scam good investment haritha lanka agarwood pl...          33  \n",
       "1  villa unit sale unawatuna sri lanka new projec...         101  \n",
       "2  need advice expert folded_hands plan podcast f...          63  \n",
       "3  confusion paddock club nugegoda halal status o...          42  \n",
       "4  tour kanneliya rain forest plan group visit ka...          35  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"cleaned_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "014f54b0-60fc-47c7-a50e-a3a3469d8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text to token IDs using the loaded tokenizer\n",
    "df_posts['tokens'] = df_posts['content_cleaned'].apply(lambda x: unigram.encode(x, out_type=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e82bcde5-908a-473b-8f1b-74bf801703bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_posts.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be2957-ee2a-430f-bb2f-6f1504151f5e",
   "metadata": {},
   "source": [
    "# Vectorization in preparation for Modelling \n",
    "\n",
    "## Both Posts + Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd9ef33-3c85-4bf4-9eb9-2f1505fa48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count shape: (38209, 214315)\n"
     ]
    }
   ],
   "source": [
    "#  Sparse Representations\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)\n",
    "X_count = count_vect.fit_transform(df['content_cleaned'])\n",
    "print(\"Count shape:\", X_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38c391c9-4c21-44d9-8e54-2f0b800efd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (38209, 214315)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)\n",
    "X_tfidf = tfidf_vect.fit_transform(df['content_cleaned'])\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac9aea8e-ed08-4e20-b049-c3e14af1f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA shape: (38209, 100)\n"
     ]
    }
   ],
   "source": [
    "#  Dense: LSA (Truncated SVD)\n",
    "\n",
    "svd_components = 100\n",
    "svd = TruncatedSVD(n_components=svd_components, random_state=42)\n",
    "X_lsa = svd.fit_transform(X_tfidf)\n",
    "print(\"LSA shape:\", X_lsa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b139cce4-c264-4444-93ad-e303a517267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dense: Word2Vec Embeddings\n",
    "\n",
    "# Tokenize text\n",
    "documents = df['content_cleaned'].astype(str).apply(lambda x: x.split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeb8434d-4d25-4ad7-bb92-e43ad2b45493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec\n",
    "w2v_dim = 300  # embedding dimension\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=documents,\n",
    "    vector_size=w2v_dim,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb68ced8-9462-41dc-8754-1942310dcec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build document embeddings\n",
    "def doc_vector(words):\n",
    "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(w2v_dim)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc93177a-6c1c-4ce0-891a-f99a187f9598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec shape: (38209, 300)\n",
      "\n",
      "All vector representations generated successfully.\n"
     ]
    }
   ],
   "source": [
    "X_w2v = np.array([doc_vector(doc) for doc in documents])\n",
    "print(\"Word2Vec shape:\", X_w2v.shape)\n",
    "\n",
    "# # Save model\n",
    "# w2v_model.save(\"word2vec.model\")\n",
    "\n",
    "print(\"\\nAll vector representations generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198e03c-fd25-4bb2-a4a9-4659056fe799",
   "metadata": {},
   "source": [
    "## Posts Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "274386bb-d81e-497b-9dc2-657e6032d88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data:  (14852, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "      <td>scam good investment haritha lanka agarwood pl...</td>\n",
       "      <td>33</td>\n",
       "      <td>[644, 15, 427, 5509, 1643, 13, 2908, 570, 7885...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "      <td>villa unit sale unawatuna sri lanka new projec...</td>\n",
       "      <td>101</td>\n",
       "      <td>[3149, 931, 966, 3058, 8, 13, 84, 369, 92, 309...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "      <td>need advice expert folded_hands plan podcast f...</td>\n",
       "      <td>63</td>\n",
       "      <td>[24, 116, 1233, 1114, 3, 905, 90, 3639, 102, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9dyw</td>\n",
       "      <td>hotstar10</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Confusion Over Paddock Club Nugegoda‚Äôs Halal S...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:28:19</td>\n",
       "      <td>confusion paddock club nugegoda halal status o...</td>\n",
       "      <td>42</td>\n",
       "      <td>[6234, 3349, 42, 3520, 1049, 2731, 5913, 1426,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9da2</td>\n",
       "      <td>prav_u</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Tour to Kanneliya Rain Forest I‚Äôm planning a g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:26:59</td>\n",
       "      <td>tour kanneliya rain forest plan group visit ka...</td>\n",
       "      <td>35</td>\n",
       "      <td>[1204, 64, 316, 152, 975, 1048, 777, 2307, 90,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id          author subreddit  \\\n",
       "0  post    new  no keyword  1otaemb  Cookiehere6969  srilanka   \n",
       "1  post    new  no keyword  1otaam5        oshan789  srilanka   \n",
       "2  post    new  no keyword  1ot9h2f    No-Leave8971  srilanka   \n",
       "3  post    new  no keyword  1ot9dyw       hotstar10  srilanka   \n",
       "4  post    new  no keyword  1ot9da2          prav_u  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "3  Confusion Over Paddock Club Nugegoda‚Äôs Halal S...    0.0           4.0   \n",
       "4  Tour to Kanneliya Rain Forest I‚Äôm planning a g...    1.0           1.0   \n",
       "\n",
       "  parent_post created_date created_time  \\\n",
       "0     no post   2025-11-10     10:33:16   \n",
       "1     no post   2025-11-10     10:26:02   \n",
       "2     no post   2025-11-10     09:33:57   \n",
       "3     no post   2025-11-10     09:28:19   \n",
       "4     no post   2025-11-10     09:26:59   \n",
       "\n",
       "                                     content_cleaned  word_count  \\\n",
       "0  scam good investment haritha lanka agarwood pl...          33   \n",
       "1  villa unit sale unawatuna sri lanka new projec...         101   \n",
       "2  need advice expert folded_hands plan podcast f...          63   \n",
       "3  confusion paddock club nugegoda halal status o...          42   \n",
       "4  tour kanneliya rain forest plan group visit ka...          35   \n",
       "\n",
       "                                              tokens  \n",
       "0  [644, 15, 427, 5509, 1643, 13, 2908, 570, 7885...  \n",
       "1  [3149, 931, 966, 3058, 8, 13, 84, 369, 92, 309...  \n",
       "2  [24, 116, 1233, 1114, 3, 905, 90, 3639, 102, 4...  \n",
       "3  [6234, 3349, 42, 3520, 1049, 2731, 5913, 1426,...  \n",
       "4  [1204, 64, 316, 152, 975, 1048, 777, 2307, 90,...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_only_df = df.copy()\n",
    "\n",
    "# Remove rows where source == \"comments\"\n",
    "post_only_df = post_only_df[post_only_df[\"source\"] != \"comment\"]\n",
    "print(\"Filtered Data: \",post_only_df.shape)\n",
    "post_only_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0775aa89-5592-4d13-893a-16fe15b17fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count shape: (14852, 97261)\n"
     ]
    }
   ],
   "source": [
    "#  Sparse Representations\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)\n",
    "count_repr = count_vect.fit_transform(post_only_df['content_cleaned'])\n",
    "print(\"Count shape:\", count_repr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "934b1707-ba4b-411a-abe3-b5877ce1c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (14852, 97261)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)\n",
    "tfidf_repr = tfidf_vect.fit_transform(post_only_df['content_cleaned'])\n",
    "print(\"TF-IDF shape:\", tfidf_repr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678e3129-7f27-4650-bb8a-fa6afbaca94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA shape: (14852, 100)\n"
     ]
    }
   ],
   "source": [
    "# Dense vectorizer: LSA (Truncated SVD)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_components = 100\n",
    "svd = TruncatedSVD(n_components=svd_components, random_state=42)\n",
    "lsa_repr = svd.fit_transform(tfidf_repr)\n",
    "print(\"LSA shape:\", lsa_repr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2adadb08-65fb-4257-95eb-5869604675c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense vectorizer: Word2Vec embeddings\n",
    "\n",
    "# Tokenize text\n",
    "documents = post_only_df['content_cleaned'].astype(str).apply(lambda x: x.split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e195b2d-657b-4d6f-8220-b56f07e71fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec\n",
    "w2v_dim = 300\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=documents,\n",
    "    vector_size=w2v_dim,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90e8ad7b-4873-4668-b35c-69dbb58ce27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build document embeddings\n",
    "def doc_vector(words):\n",
    "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(w2v_dim)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24a31210-05a6-46c6-a83f-919a02acda1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec shape: (14852, 300)\n",
      "\n",
      "All vector representations generated successfully.\n"
     ]
    }
   ],
   "source": [
    "w2v_repr = np.array([doc_vector(doc) for doc in documents])\n",
    "print(\"Word2Vec shape:\", w2v_repr.shape)\n",
    "\n",
    "print(\"\\nAll vector representations generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72cc22-f6f3-4bf3-a945-06ffd91b26e7",
   "metadata": {},
   "source": [
    "## **Justification of Vector Representations**\n",
    "\n",
    "### **1. Sparse Representations**\n",
    "\n",
    "**a) Count Vector (Bag-of-Words)**\n",
    "\n",
    "* Choice: CountVectorizer with unigrams and bigrams.\n",
    "* Reason:\n",
    "  * Captures raw frequency of words/phrases in each document.\n",
    "  * Bigrams help detect short phrases and context (**‚Äúnot good‚Äù**, **‚Äúhigh risk‚Äù**).\n",
    "  * Sparse format is memory-efficient for high-dimensional data.\n",
    "* **Dimension:** `(48028, 172884)` ‚Üí 48k posts √ó 172k vocabulary features.\n",
    "\n",
    "**b) TF-IDF Vector**\n",
    "\n",
    "* **Choice:** TfidfVectorizer with unigrams and bigrams.\n",
    "* **Reason:**\n",
    "\n",
    "  * Improves on raw counts by down-weighting very common words and up-weighting discriminative terms.\n",
    "  * Reduces noise and highlights informative features for clustering/classification.\n",
    "* **Dimension:** `(48028, 172884)` ‚Üí same vocabulary, different feature weighting.\n",
    "\n",
    "### **2. Dense Representations**\n",
    "\n",
    "**a) LSA (Truncated SVD)**\n",
    "\n",
    "* **Choice:** Reduce TF-IDF matrix to 100 latent dimensions.\n",
    "* **Reason:**\n",
    "\n",
    "  * Captures **latent semantic structure** rather than raw word counts.\n",
    "  * Reduces dimensionality from 172k ‚Üí 100, making downstream models faster and less prone to overfitting.\n",
    "  * Dense vectors allow similarity-based clustering (e.g., KMeans) to work better.\n",
    "* **Dimension:** `(48028, 100)` ‚Üí 48k documents √ó 100 latent features.\n",
    "\n",
    "**b) Word2Vec (Average Word Embeddings)**\n",
    "\n",
    "* **Choice:** Train 300-dimensional word embeddings and average per document.\n",
    "* **Reason:**\n",
    "\n",
    "  * Captures **semantic meaning** of words and documents, not just frequency.\n",
    "  * Dense, compact, and suitable for clustering or classification.\n",
    "  * Complementary to LSA because it uses **contextual similarity** rather than linear algebra on term-frequency.\n",
    "* **Dimension:** `(48028, 300)` ‚Üí 48k documents √ó 300-dimensional dense vectors.\n",
    "\n",
    "\n",
    " **Summary Table**\n",
    "\n",
    "| Representation | Type   | Dimensionality  | Justification                                                                    |\n",
    "| -------------- | ------ | --------------- | -------------------------------------------------------------------------------- |\n",
    "| Count          | Sparse | (48028, 172884) | Captures raw term frequency with unigrams + bigrams; good baseline for ML        |\n",
    "| TF-IDF         | Sparse | (48028, 172884) | Highlights informative terms, reduces noise from frequent words                  |\n",
    "| LSA            | Dense  | (48028, 100)    | Reduces dimensionality, captures latent semantic structure for better clustering |\n",
    "| Word2Vec       | Dense  | (48028, 300)    | Captures semantic meaning; dense embeddings improve similarity-based tasks       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc6c01-0c40-47d8-9e29-eb4de63fe1c2",
   "metadata": {},
   "source": [
    "# Categorization of the dataset based on their content by using document clustering\n",
    "## Categorize the whole dataset (Posts + Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4bc514-62f7-49c4-926d-eb2646b7bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Word2Vec vectors\n",
    "X_input = X_w2v  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd355c-ed4c-4811-934c-5e7028a02f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to unit length for spherical k-means\n",
    "X_norm = normalize(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f0538-26f7-4b55-b17d-b7e46e079cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "sil_scores = []\n",
    "k_values = range(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12fe88-db15-486d-b092-a6e8d54255f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = km.fit_predict(X_norm)\n",
    "    \n",
    "    # SSE on normalized vectors\n",
    "    sse.append(km.inertia_)\n",
    "    \n",
    "    # Silhouette score using cosine  metric\n",
    "    sil_scores.append(silhouette_score(X_norm, labels, metric='cosine'))\n",
    "\n",
    "print(\"Silhouette Scores:\", sil_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8a9c6-1662-4cfd-a7d3-676cc773b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Elbow plot\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title('Elbow Method (Spherical KMeans)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "# Silhouette plot\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(k_values, sil_scores, marker='o', color='orange')\n",
    "plt.title('Silhouette Score (Cosine)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b587251-a99b-490a-9f79-f3e7bacb8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "\n",
    "# Number of topics/clusters\n",
    "best_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745a9f2-f993-492b-b75d-8f8fb3723ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize W2V vectors (spherical clustering works better)\n",
    "X_norm = normalize(X_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a532e4-565e-4a6f-a11b-ff9d24d110d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_w2v = KMeans(\n",
    "    n_clusters=best_k,\n",
    "    random_state=42,\n",
    "    n_init=20,\n",
    "    max_iter=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07938e-7a05-4975-92f3-fe1b06f5623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_labels = kmeans_w2v.fit_predict(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703821a-3b4a-4500-9637-5ffb88222b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = w2v_labels\n",
    "\n",
    "print(\"Word2Vec Topic Distribution:\")\n",
    "unique, counts = np.unique(w2v_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d09c8-61ab-446b-aa94-398f03bd4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Words to Interpret Topics \n",
    "\n",
    "word_vectors = w2v_model.wv.vectors\n",
    "word_list = w2v_model.wv.index_to_key\n",
    "\n",
    "kmeans_words = KMeans(\n",
    "    n_clusters=best_k,\n",
    "    random_state=42,\n",
    "    n_init=20\n",
    ").fit(word_vectors)\n",
    "\n",
    "word_cluster_labels = kmeans_words.labels_\n",
    "\n",
    "# Group words by cluster\n",
    "topic_terms = {}\n",
    "for word, label in zip(word_list, word_cluster_labels):\n",
    "    topic_terms.setdefault(label, []).append(word)\n",
    "\n",
    "# Print top words (20 most common) for each topic\n",
    "for t in range(best_k):\n",
    "    print(f\"\\nWord2Vec Topic {t}:\")\n",
    "    print(\", \".join(topic_terms[t][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8983a5-483a-4394-9898-c28f96eac520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Topic-Specific Keyword Dictionaries\n",
    "\n",
    "topic_keywords = {\n",
    "    \"Education & Work\": [\n",
    "        \"education\",\"university\",\"degree\",\"student\",\"students\",\"school\",\n",
    "        \"campus\",\"exam\",\"exams\",\"teacher\",\"lecture\",\"course\",\"job\",\n",
    "        \"career\",\"work\",\"office\"\n",
    "    ],\n",
    "    \"Politics\": [\n",
    "        \"government\",\"election\",\"minister\",\"president\",\"politics\",\n",
    "        \"policy\",\"law\",\"parliament\",\"vote\",\"public\",\"political\"\n",
    "    ],\n",
    "    \"Tourism & Living\": [\n",
    "        \"travel\",\"tourist\",\"hotel\",\"trip\",\"flight\",\"destination\",\n",
    "        \"beach\",\"tourism\",\"restaurant\",\"food\",\"city\",\"living\"\n",
    "    ],\n",
    "    \"Technology\": [\n",
    "        \"software\",\"ai\",\"machine\",\"tech\",\"computer\",\"data\",\"app\",\n",
    "        \"mobile\",\"digital\",\"systems\",\"internet\",\"programming\"\n",
    "    ],\n",
    "    \"Culture\": [\n",
    "        \"culture\",\"tradition\",\"festival\",\"language\",\"music\",\"dance\",\n",
    "        \"heritage\",\"religion\",\"custom\"\n",
    "    ],\n",
    "    \"Social Issues\": [\n",
    "        \"crime\",\"violence\",\"protest\",\"rights\",\"poverty\",\"health\",\n",
    "        \"mental\",\"community\",\"social\",\"gender\",\"harassment\"\n",
    "    ],\n",
    "    \"Economy\": [\n",
    "        \"money\",\"finance\",\"bank\",\"salary\",\"business\",\"market\",\n",
    "        \"economic\",\"trade\",\"inflation\",\"budget\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert lists to lowercase for matching\n",
    "topic_keywords = {k: [w.lower() for w in v] for k, v in topic_keywords.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a53a3d-316d-4eef-9bba-0f652381ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper to score clusters\n",
    "\n",
    "def score_cluster(words_in_cluster, topic_keywords):\n",
    "    \"\"\"\n",
    "    Returns a dictionary: {topic_name: match_score}\n",
    "    Score = how many keywords appear inside the cluster‚Äôs word list\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    lower_cluster_words = [w.lower() for w in words_in_cluster]\n",
    "\n",
    "    for topic, keyword_list in topic_keywords.items():\n",
    "        score = sum(1 for kw in keyword_list if kw in lower_cluster_words)\n",
    "        scores[topic] = score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4774242-9b22-46fe-977f-f5fd5b82104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Automatically assign a topic to each cluster\n",
    "\n",
    "cluster_names = {}            # final {cluster_id: name}\n",
    "used_topic_names = set()      # prevents duplicates\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    words = topic_terms[cluster_id]     # your list of words in the cluster\n",
    "\n",
    "    # Compute scores for this cluster\n",
    "    scores = score_cluster(words, topic_keywords)\n",
    "\n",
    "    # Sort topics by score (descending)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    assigned_name = None\n",
    "    for topic, score in sorted_scores:\n",
    "        if score > 0 and topic not in used_topic_names:\n",
    "            assigned_name = topic\n",
    "            used_topic_names.add(topic)\n",
    "            break\n",
    "\n",
    "    # If no keyword matches ‚Üí assign fallback cluster label\n",
    "    if assigned_name is None:\n",
    "        assigned_name = f\"Misc_{cluster_id}\"\n",
    "\n",
    "    cluster_names[cluster_id] = assigned_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012655eb-31b8-4873-9760-89fd735e1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Final Cluster Names :\")\n",
    "for cid, name in cluster_names.items():\n",
    "    print(f\"Cluster {cid}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d6e5b-3997-4ffa-b0b1-38751afc32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters in a scatterplot\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    "    perplexity=40,\n",
    "    max_iter=1000    \n",
    ")\n",
    "\n",
    "X_2d = tsne.fit_transform(X_w2v)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    idx = (labels == cluster_id)  # labels from kmeans\n",
    "    plt.scatter(\n",
    "        X_2d[idx, 0],\n",
    "        X_2d[idx, 1],\n",
    "        s=12,\n",
    "        alpha=0.7,\n",
    "        label=f\"Cluster {cluster_id}\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Word2Vec Document Clusters (t-SNE)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce28e12-2386-468e-9f0b-84654fad565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    "    perplexity=40,\n",
    "    max_iter=1000    \n",
    ")\n",
    "\n",
    "X_2d = tsne.fit_transform(X_w2v)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    idx = (w2v_labels == cluster_id)  # labels from kmeans\n",
    "    plt.scatter(\n",
    "        X_2d[idx, 0],\n",
    "        X_2d[idx, 1],\n",
    "        s=12,\n",
    "        alpha=0.7,\n",
    "        label=f\"Cluster {cluster_id}\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Word2Vec Document Clusters (t-SNE)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524329c1-7ac1-4661-ae41-e8d80e81dd65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad607610-4696-44b6-a08a-dde31ee0858f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382cea53-8948-4f5c-a362-67da868e5611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GPU Env",
   "language": "python",
   "name": "gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
