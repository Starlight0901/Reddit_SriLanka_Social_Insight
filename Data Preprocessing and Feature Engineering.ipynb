{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097bf05-6241-43f3-934c-0ca02218e8f6",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672397e2-7d58-4142-a3b2-b5dd095428d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd7c-fc07-4994-8853-bcad1c6ed0ac",
   "metadata": {},
   "source": [
    "# Commit to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b657452-1d16-4c01-9ed3-e38dfd5dc90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Data Preprocessing and Feature Engineering.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 360f8aa] Data Preprocessing Intializing\n",
      " 1 file changed, 887 insertions(+), 6 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/Starlight0901/Reddit_SriLanka_Social_Insight.git\n",
      "   7c91ccc..360f8aa  main -> main\n"
     ]
    }
   ],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"Data Preprocessing Intializing\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260df94-6335-4131-b1c5-962708bb99d7",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141bad06-47a7-4b41-bbfd-0e62b5c7de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a289-471b-440f-8e73-57d9c0c0120a",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237f29ed-ddac-4873-b8f7-b54a183f4f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_17396\\3966237626.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source keyword       id   created_utc             author subreddit  \\\n",
       "0  post    new     NaN  1otaemb  1.762771e+09     Cookiehere6969  srilanka   \n",
       "1  post    new     NaN  1otaam5  1.762770e+09           oshan789  srilanka   \n",
       "2  post    new     NaN  1ot9w1v  1.762769e+09          mgssjjsks  srilanka   \n",
       "3  post    new     NaN  1ot9kwe  1.762768e+09  Critical_Rise_exe  srilanka   \n",
       "4  post    new     NaN  1ot9h2f  1.762767e+09       No-Leave8971  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                  Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "\n",
       "  parent_post  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc654212-751a-4707-ae63-580c3f5c958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70861, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset shape:\", df_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115c4822-97df-4205-a720-40df51d9dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates based on 'content':\n",
      "Empty DataFrame\n",
      "Columns: [type, source, keyword, id, created_utc, author, subreddit, content, score, num_comments, parent_post]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on a specific column\n",
    "duplicates_name = df_posts.duplicated(subset=['content'])\n",
    "print(\"\\nDuplicates based on 'content':\")\n",
    "print(df_posts[duplicates_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8c3f2e-a6a3-4abf-9a83-44d8c0527e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971e6205-4161-488b-95aa-fe05e2398424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc55767c-f6b5-4d1b-a33d-1d52e211ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword             0\n",
      "id                  0\n",
      "created_utc         0\n",
      "author              0\n",
      "subreddit           0\n",
      "content             1\n",
      "score               0\n",
      "num_comments    52021\n",
      "parent_post         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id   created_utc             author  \\\n",
       "0  post    new  no keyword  1otaemb  1.762771e+09     Cookiehere6969   \n",
       "1  post    new  no keyword  1otaam5  1.762770e+09           oshan789   \n",
       "2  post    new  no keyword  1ot9w1v  1.762769e+09          mgssjjsks   \n",
       "3  post    new  no keyword  1ot9kwe  1.762768e+09  Critical_Rise_exe   \n",
       "4  post    new  no keyword  1ot9h2f  1.762767e+09       No-Leave8971   \n",
       "\n",
       "  subreddit                                            content  score  \\\n",
       "0  srilanka  Is this a Scam or good investment? Haritha Lan...    2.0   \n",
       "1  srilanka  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0   \n",
       "2  srilanka  Whats your hot take on Sri Lanka as the title ...    3.0   \n",
       "3  srilanka                  Is the rs.11 deals real in Daraz?    1.0   \n",
       "4  srilanka  Need advice from the experts üôè [](https://www....    2.0   \n",
       "\n",
       "   num_comments parent_post  \n",
       "0           1.0     no post  \n",
       "1           0.0     no post  \n",
       "2           8.0     no post  \n",
       "3           3.0     no post  \n",
       "4           0.0     no post  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill 'keyword' missing values\n",
    "df_posts['keyword'] = df_posts['keyword'].fillna('no keyword')\n",
    "\n",
    "# Fill 'author' missing values\n",
    "df_posts['author'] = df_posts['author'].fillna('no author')\n",
    "\n",
    "# Fill 'score' missing values with the median\n",
    "median_score = df_posts['score'].median()\n",
    "df_posts['score'] = df_posts['score'].fillna(median_score)\n",
    "\n",
    "# Fill 'num_comments' missing values with 0 only where source == 'comments'\n",
    "mask = df_posts['source'] == 'comments'\n",
    "df_posts.loc[mask, 'num_comments'] = df_posts.loc[mask, 'num_comments'].fillna(0)\n",
    "\n",
    "# Fill 'parent_post' missing values\n",
    "df_posts['parent_post'] = df_posts['parent_post'].fillna('no post')\n",
    "\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) \n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9bf730-1b88-45fc-8826-c2191464a088",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['created_datetime'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Drop the original 'created_utc' column\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df_posts\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m df_posts\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m df_posts\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['created_datetime'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# converting date column into a readable format\n",
    "df_posts['created_date'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.date\n",
    "df_posts['created_time'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.time\n",
    "\n",
    "# Drop the original 'created_utc' column\n",
    "df_posts.drop(columns=['created_utc'], inplace=True)\n",
    "df_posts.drop(columns=['created_datetime'], inplace=True)\n",
    "\n",
    "df_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867aa08f-21fe-4469-b4e9-c72d3fb89d57",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Example: Load your data\n",
    "# df = pd.read_csv('reddit_posts.csv')  # Assuming your text column is 'post_text'\n",
    "\n",
    "# For demonstration\n",
    "data = {\n",
    "    'post_text': [\n",
    "        \"Check this out! https://example.com üòä #fun\",\n",
    "        \"I totally agree with you!!!\",\n",
    "        \"Short post\",\n",
    "        \"This is a very long post \" * 500,  # simulating a long post\n",
    "        \"Normal post with some emojis üòé and a URL http://reddit.com\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Cleaning and preprocessing function\n",
    "def clean_text(text):\n",
    "    text = str(text)  # Ensure text is string\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove emojis and non-ASCII characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Remove Reddit/platform-specific metadata (like u/username, r/subreddit)\n",
    "    text = re.sub(r'u\\/\\w+|r\\/\\w+', '', text)\n",
    "    # Remove extra whitespace, line breaks, tabs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['post_text'].apply(clean_text)\n",
    "\n",
    "# Compute word counts\n",
    "df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Filter posts by word count (e.g., 25 <= words <= 1000)\n",
    "lower_threshold = 25\n",
    "upper_threshold = 1000\n",
    "df_filtered = df[(df['word_count'] >= lower_threshold) & (df['word_count'] <= upper_threshold)].copy()\n",
    "\n",
    "# Combine all cleaned posts into one corpus\n",
    "corpus = ' '.join(df_filtered['cleaned_text'].tolist())\n",
    "\n",
    "# Tokenize corpus and count unique words\n",
    "tokens = corpus.split()\n",
    "total_words = len(tokens)\n",
    "unique_words = len(set(tokens))\n",
    "\n",
    "# Report results\n",
    "print(f\"Number of posts after filtering: {len(df_filtered)}\")\n",
    "print(f\"Total words in corpus: {total_words}\")\n",
    "print(f\"Unique words in corpus: {unique_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f705a-fb07-4324-83f1-f3198e3ebb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
