{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097bf05-6241-43f3-934c-0ca02218e8f6",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672397e2-7d58-4142-a3b2-b5dd095428d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ff0e2-6eae-4c89-a7f1-41e6072aa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ee78a-983f-40b7-8add-c947f48b6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c4bd2-1de9-402c-9a5b-6cd6f095d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f84da2-aabd-4299-9ee5-c1e85b96afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2421d06-697f-4030-9c14-a3de0ee36f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd7c-fc07-4994-8853-bcad1c6ed0ac",
   "metadata": {},
   "source": [
    "# Commit to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b657452-1d16-4c01-9ed3-e38dfd5dc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"comparing tokenization schemes,\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260df94-6335-4131-b1c5-962708bb99d7",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905cacb-0df9-4190-a3a6-31539a46e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141bad06-47a7-4b41-bbfd-0e62b5c7de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sentencepiece as spm\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from langdetect import detect\n",
    "import langdetect\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy loaded OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a289-471b-440f-8e73-57d9c0c0120a",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f29ed-ddac-4873-b8f7-b54a183f4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc654212-751a-4707-ae63-580c3f5c958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset shape:\", df_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c4822-97df-4205-a720-40df51d9dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on a specific column\n",
    "duplicates_name = df_posts.duplicated(subset=['content'])\n",
    "print(\"\\nDuplicates based on 'content':\")\n",
    "print(df_posts[duplicates_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c3f2e-a6a3-4abf-9a83-44d8c0527e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e6205-4161-488b-95aa-fe05e2398424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55767c-f6b5-4d1b-a33d-1d52e211ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'keyword' missing values\n",
    "df_posts['keyword'] = df_posts['keyword'].fillna('no keyword')\n",
    "\n",
    "# Fill 'author' missing values\n",
    "df_posts['author'] = df_posts['author'].fillna('no author')\n",
    "\n",
    "# Fill 'score' missing values with the median\n",
    "median_score = df_posts['score'].median()\n",
    "df_posts['score'] = df_posts['score'].fillna(median_score)\n",
    "\n",
    "# Fill 'num_comments' missing values with 0 only where source == 'comments'\n",
    "mask = df_posts['source'] == 'comments'\n",
    "df_posts.loc[mask, 'num_comments'] = df_posts.loc[mask, 'num_comments'].fillna(0)\n",
    "\n",
    "# Fill 'parent_post' missing values\n",
    "df_posts['parent_post'] = df_posts['parent_post'].fillna('no post')\n",
    "\n",
    "# Drop rows where 'content' is missing (only 1 row)\n",
    "df_posts = df_posts.dropna(subset=['content'])\n",
    "# reset the index\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) \n",
    "\n",
    "print(\"\\n Dataset shape:\", df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bf730-1b88-45fc-8826-c2191464a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date column into a readable format\n",
    "df_posts['created_date'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.date\n",
    "df_posts['created_time'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.time\n",
    "\n",
    "# Drop the original 'created_utc' column\n",
    "df_posts.drop(columns=['created_utc'], inplace=True)\n",
    "df_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4e95-d80e-45a3-a350-8c9920010d27",
   "metadata": {},
   "source": [
    "### Drop non-english data (sinhala and tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f705a-fb07-4324-83f1-f3198e3ebb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Create a new column for language\n",
    "df_posts['language'] = df_posts['content'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0387c0-c623-44f6-bd16-9e0c725d21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only English posts\n",
    "df_posts = df_posts[df_posts['language'] == 'en'].copy()\n",
    "\n",
    "# Drop the language column\n",
    "df_posts.drop(columns=['language'], inplace=True)\n",
    "print(df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269d332-f121-47a5-826a-6d39a9fa12f5",
   "metadata": {},
   "source": [
    "# Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65b95f-5a8c-4d3f-9bdf-7bda2e0014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc3251-fc5e-4a61-bd44-b59c78f83070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3002fcd-372b-4b0a-b3f4-b0eb00c02a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace emojis with textual description\n",
    "def emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761b816-5a43-4f9f-8145-af51db83db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert emojis to text\n",
    "    text = emoji_to_text(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove Reddit platform metadata (u/username, r/subreddit)\n",
    "    text = re.sub(r'u\\/\\w+', '', text)\n",
    "    text = re.sub(r'r\\/\\w+', '', text)\n",
    "    \n",
    "    # Remove escape sequences and extra whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove punctuation (except within words like can't, won't)\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Join back into string\n",
    "    cleaned_text = \" \".join(lemmatized)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb036458-ecda-48d1-b53a-1d5414f508de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function\n",
    "df_posts['content_cleaned'] = df_posts['content'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02e9b7-e3e9-4f47-bb7e-f26e4a88cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word count column\n",
    "df_posts[\"word_count\"] = df_posts[\"content_cleaned\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot histogram of word counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df_posts[\"word_count\"])\n",
    "plt.xlabel(\"Word Count per Document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Word Counts in df_posts['content_cleaned']\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b487d6-7a7e-4f16-8a76-0c52afd72903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute recommended thresholds using quantiles\n",
    "lower_threshold = df_posts[\"word_count\"].quantile(0.3)\n",
    "upper_threshold = df_posts[\"word_count\"].quantile(0.95)\n",
    "\n",
    "print(\"Lower Threshold (10th percentile):\", lower_threshold)\n",
    "print(\"Upper Threshold (95th percentile):\", upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6499c39-7245-4f9d-9f56-7c490af00edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_backup = df_posts.copy()\n",
    "df_posts = df_posts[\n",
    "    (df_posts[\"word_count\"] >= lower_threshold) &\n",
    "    (df_posts[\"word_count\"] <= upper_threshold)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ba0d7-7d88-410f-bfc9-69585628f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Report corpus statistics\n",
    "all_text = \" \".join(df_posts['content_cleaned']) \n",
    "all_words = all_text.split()\n",
    "total_words = len(all_words)\n",
    "unique_words = len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af03fd-3942-450f-b484-24ea6c93d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total words in cleaned corpus: {total_words}\")\n",
    "print(f\"Unique words in cleaned corpus: {unique_words}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c7e78-8876-4cb9-9dd6-3e1dccc7eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3364-07aa-48ea-8ecb-47017be5ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ebeac-28b9-4c21-a126-4420cd12afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode characters using NFKC.\n",
    "    Converts full-width characters, combined characters, and compatibility characters \n",
    "    into a consistent canonical form.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "# Apply normalization\n",
    "df_posts[\"content_cleaned\"] = (\n",
    "    df_posts[\"content_cleaned\"]\n",
    "    .astype(str)\n",
    "    .apply(normalize_unicode)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9235e7-7031-415e-b519-03b6cbd2bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f9436-577f-4e9a-a4ff-10023c80b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "\n",
    "df_posts.to_csv(\"cleaned_Posts_Data.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a3204-8c0e-47bf-bd7c-b81f36be1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df_posts = pd.read_csv(\"cleaned_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2d644-226f-4eb3-a4b9-d7c9730f2501",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "## Traditional Word-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7bea7-fc9e-4897-9276-97c8df0390b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level tokenizer\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7154f20-a194-4e6d-9b98-60a8fb5668ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts[\"processed\"] = df_posts[\"content_cleaned\"].apply(word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50e6cd-d4b4-412f-958a-3fb10e781501",
   "metadata": {},
   "source": [
    "## Prepare corpus for SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee33c27-4779-4cc3-8ad3-3a0a616de735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for SentencePiece\n",
    "corpus_file = \"corpus.txt\"\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in df_posts[\"content_cleaned\"]:\n",
    "        f.write(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911460ac-7497-45cb-85f1-ed1c79d2e7e9",
   "metadata": {},
   "source": [
    "### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22435aa6-f316-4096-b262-d4cd2dd6f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=bpe --vocab_size=8000 --model_type=bpe\"\n",
    ")\n",
    "bpe = spm.SentencePieceProcessor()\n",
    "bpe.load(\"bpe.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218db90-4597-4643-a8be-fd8402cf3742",
   "metadata": {},
   "source": [
    "### Unigram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1309a67-1c85-4642-99bb-8a3d779e0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=unigram --vocab_size=8000 --model_type=unigram\"\n",
    ")\n",
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(\"unigram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efc7d6-2799-49b2-aa41-f98133f45dc1",
   "metadata": {},
   "source": [
    "## Compare Tokenization Schemes Impact using Corpus Statistics\n",
    "\n",
    "Calculating:\n",
    "- vocabulary size\n",
    "- average tokens per document\n",
    "- total tokens\n",
    "- rare token frequency (<3 occurrences)\n",
    "- OOV rate (for word-based tokenizers only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc559e0-ef35-44b1-a305-7b918ad63139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus statistics per tokenizer\n",
    "\n",
    "def compute_stats(tokenizer_fn, tokenizer_name):\n",
    "    all_tokens = []\n",
    "    doc_lengths = []\n",
    "\n",
    "    for text in df_posts[\"content_cleaned\"]:\n",
    "        tokens = tokenizer_fn(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        doc_lengths.append(len(tokens))\n",
    "\n",
    "    vocab = set(all_tokens)\n",
    "    counter = collections.Counter(all_tokens)\n",
    "\n",
    "    return {\n",
    "        \"Tokenizer\": tokenizer_name,\n",
    "        \"Vocabulary Size\": len(vocab),\n",
    "        \"Total Tokens\": len(all_tokens),\n",
    "        \"Avg Tokens per Doc\": sum(doc_lengths)/len(doc_lengths),\n",
    "        \"Rare Tokens (<3)\": sum(1 for t,c in counter.items() if c < 3)\n",
    "    }\n",
    "\n",
    "stats_word = compute_stats(word_tokenizer, \"Word+Lemmatization\")\n",
    "stats_bpe = compute_stats(lambda t: bpe.encode(t, out_type=str), \"BPE\")\n",
    "stats_unigram = compute_stats(lambda t: unigram.encode(t, out_type=str), \"Unigram LM\")\n",
    "\n",
    "stats_df = pd.DataFrame([stats_word, stats_bpe, stats_unigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98091a65-06f3-4fdf-a1e1-12e30aa6d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee644717-5b74-4dfb-b83f-6920a2d0a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to Tokenizer (for easier plotting)\n",
    "plot_df = stats_df.set_index(\"Tokenizer\")\n",
    "\n",
    "# Plot each metric\n",
    "plot_df.plot(kind=\"bar\", figsize=(10,6))\n",
    "plt.title(\"Comparison of Tokenizer Statistics\")\n",
    "plt.xlabel(\"Tokenizer\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef04a8f-a35d-4f63-a1a6-ff2c18d2c02c",
   "metadata": {},
   "source": [
    "The tokenization analysis shows clear differences between the three approaches: Word+Lemmatization, BPE, and Unigram LMâ€”and the results strongly align with theoretical expectations. The word-level tokenizer produced an extremely large vocabulary of 47,222 items and a very high number of rare tokens (31,098), even after lemmatization and stopword removal. This indicates severe sparsity: each spelling variant, slang term, and morphological form becomes a separate token, making the representation unstable and prone to poor generalization. In contrast, BPE reduced the vocabulary size to 8,887 and dropped rare tokens to just 1,023, demonstrating its ability to decompose infrequent or noisy words into reusable subword units. This leads to a more robust and consistent representation, though at the cost of slightly longer token sequences, which is expected for subword models. The Unigram LM tokenizer achieved similar vocabulary compression (8,962 tokens) but yielded the lowest number of rare tokens (855), reflecting its probabilistic approach to selecting the most efficient and informative subword units. Overall, the results validate the progression predicted by NLP literature: word-level tokenization is the least efficient, BPE offers substantial improvements, and Unigram LM provides the most balanced and linguistically consistent tokenization strategy for noisy text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44a405-97c1-4b21-9468-7cbccc5bb2ba",
   "metadata": {},
   "source": [
    "# Evaluate Tokenization Schemes and effectiveness for LLM Tasks\n",
    "\n",
    "## LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09580281-328b-4ede-ae4f-06f7b61bede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU Training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414332c-d7e4-46fd-9940-014f0662f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, token_ids, seq_len=30):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+1+self.seq_len], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646096c3-06e9-4513-9a34-86333b9b8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM LM model\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        o, _ = self.lstm(e)\n",
    "        return self.fc(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaba90d-1f53-4b0f-8f5d-31ff677eeb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_lm(model, dataloader, epochs=3, lr=0.001):\n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optim.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0044e-71b7-4782-8afd-dcfdfd9f45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "def perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item() * x.numel()\n",
    "            total_tokens += x.numel()\n",
    "    return torch.exp(torch.tensor(total_loss / total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91890ad-e79e-456e-a8d0-36ba1e2e1ae6",
   "metadata": {},
   "source": [
    "### Train LSTMs per Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe1b9-d201-42ec-955c-04b0b4e6a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "SEQ = 30\n",
    "BATCH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36eecd-7cac-4b7b-82b2-07df6650cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level Tokenizer\n",
    "word_vocab = {w:i for i,w in enumerate(set(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer)\n",
    ")))}\n",
    "word_id = lambda toks: [word_vocab[t] for t in toks if t in word_vocab]\n",
    "\n",
    "word_ids = list(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer).apply(word_id)\n",
    "))\n",
    "word_dl = DataLoader(LMDataset(word_ids, SEQ), batch_size=BATCH)\n",
    "lm_word = LSTMLM(len(word_vocab)).to(device)\n",
    "lm_word = train_lm(lm_word, word_dl)\n",
    "pp_word = perplexity(lm_word, word_dl)\n",
    "print(\"Word-level LM perplexity:\", pp_word.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d62b70-6322-4c02-8cca-f1d46c30da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE\n",
    "bpe_ids = [i for t in df_posts[\"content_cleaned\"] for i in bpe.encode(t)]\n",
    "bpe_dl = DataLoader(LMDataset(bpe_ids, SEQ), batch_size=BATCH)\n",
    "lm_bpe = LSTMLM(bpe.get_piece_size()).to(device)\n",
    "lm_bpe = train_lm(lm_bpe, bpe_dl)\n",
    "pp_bpe = perplexity(lm_bpe, bpe_dl)\n",
    "print(\"BPE LM perplexity:\", pp_bpe.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8cef4-9e18-4902-a76e-421cfd7e8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram\n",
    "uni_ids = [i for t in df_posts[\"content_cleaned\"] for i in unigram.encode(t)]\n",
    "uni_dl = DataLoader(LMDataset(uni_ids, SEQ), batch_size=BATCH)\n",
    "lm_uni = LSTMLM(unigram.get_piece_size()).to(device)\n",
    "lm_uni = train_lm(lm_uni, uni_dl) \n",
    "pp_uni = perplexity(lm_uni, uni_dl)\n",
    "print(\"Unigram LM perplexity:\", pp_uni.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062470c-1250-4db1-9436-702e6239d5fc",
   "metadata": {},
   "source": [
    "### Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ab0b5-2234-455d-80e7-a9463431edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame({\n",
    "    \"Tokenizer\": [\"Word+Lemmatization\", \"BPE\", \"Unigram\"],\n",
    "    \"Vocabulary Size\": [stats_word[\"Vocabulary Size\"], stats_bpe[\"Vocabulary Size\"], stats_unigram[\"Vocabulary Size\"]],\n",
    "    \"Total Tokens\": [stats_word[\"Total Tokens\"], stats_bpe[\"Total Tokens\"], stats_unigram[\"Total Tokens\"]],\n",
    "    \"Avg Tokens per Doc\": [stats_word[\"Avg Tokens per Doc\"], stats_bpe[\"Avg Tokens per Doc\"], stats_unigram[\"Avg Tokens per Doc\"]],\n",
    "    \"Rare Tokens (<3)\": [stats_word[\"Rare Tokens (<3)\"], stats_bpe[\"Rare Tokens (<3)\"], stats_unigram[\"Rare Tokens (<3)\"]],\n",
    "    \"Perplexity\": [pp_word, pp_bpe, pp_uni]\n",
    "})\n",
    "print(final_results)\n",
    "\n",
    "# comparison = pd.DataFrame({\n",
    "#     \"Tokenizer\": [\"Word\", \"BPE\", \"Unigram\"],\n",
    "#     \"Vocabulary Size\": [len(word_vocab), bpe.get_piece_size(), unigram.get_piece_size()],\n",
    "#     \"Perplexity\": [pp_word.item(), pp_bpe.item(), pp_uni.item()]\n",
    "# })\n",
    "# print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37271053-ba84-4e6a-b42d-c8b07aab7474",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
