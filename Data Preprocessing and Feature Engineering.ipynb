{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097bf05-6241-43f3-934c-0ca02218e8f6",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672397e2-7d58-4142-a3b2-b5dd095428d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk spacy emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ff0e2-6eae-4c89-a7f1-41e6072aa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ee78a-983f-40b7-8add-c947f48b6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd7c-fc07-4994-8853-bcad1c6ed0ac",
   "metadata": {},
   "source": [
    "# Commit to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b657452-1d16-4c01-9ed3-e38dfd5dc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"Data text Preprocessing and cleaning\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260df94-6335-4131-b1c5-962708bb99d7",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141bad06-47a7-4b41-bbfd-0e62b5c7de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import sentencepiece as spm\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from langdetect import detect\n",
    "import langdetect\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a289-471b-440f-8e73-57d9c0c0120a",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f29ed-ddac-4873-b8f7-b54a183f4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc654212-751a-4707-ae63-580c3f5c958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset shape:\", df_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c4822-97df-4205-a720-40df51d9dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on a specific column\n",
    "duplicates_name = df_posts.duplicated(subset=['content'])\n",
    "print(\"\\nDuplicates based on 'content':\")\n",
    "print(df_posts[duplicates_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c3f2e-a6a3-4abf-9a83-44d8c0527e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e6205-4161-488b-95aa-fe05e2398424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55767c-f6b5-4d1b-a33d-1d52e211ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'keyword' missing values\n",
    "df_posts['keyword'] = df_posts['keyword'].fillna('no keyword')\n",
    "\n",
    "# Fill 'author' missing values\n",
    "df_posts['author'] = df_posts['author'].fillna('no author')\n",
    "\n",
    "# Fill 'score' missing values with the median\n",
    "median_score = df_posts['score'].median()\n",
    "df_posts['score'] = df_posts['score'].fillna(median_score)\n",
    "\n",
    "# Fill 'num_comments' missing values with 0 only where source == 'comments'\n",
    "mask = df_posts['source'] == 'comments'\n",
    "df_posts.loc[mask, 'num_comments'] = df_posts.loc[mask, 'num_comments'].fillna(0)\n",
    "\n",
    "# Fill 'parent_post' missing values\n",
    "df_posts['parent_post'] = df_posts['parent_post'].fillna('no post')\n",
    "\n",
    "# Drop rows where 'content' is missing (only 1 row)\n",
    "df_posts = df_posts.dropna(subset=['content'])\n",
    "# reset the index\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) \n",
    "\n",
    "print(\"\\n Dataset shape:\", df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bf730-1b88-45fc-8826-c2191464a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date column into a readable format\n",
    "df_posts['created_date'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.date\n",
    "df_posts['created_time'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.time\n",
    "\n",
    "# Drop the original 'created_utc' column\n",
    "df_posts.drop(columns=['created_utc'], inplace=True)\n",
    "df_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4e95-d80e-45a3-a350-8c9920010d27",
   "metadata": {},
   "source": [
    "### Drop non-english data (sinhala and tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f705a-fb07-4324-83f1-f3198e3ebb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Create a new column for language\n",
    "df_posts['language'] = df_posts['content'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0387c0-c623-44f6-bd16-9e0c725d21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only English posts\n",
    "df_posts = df_posts[df_posts['language'] == 'en'].copy()\n",
    "\n",
    "# Drop the language column\n",
    "df_posts.drop(columns=['language'], inplace=True)\n",
    "print(df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868e0ae-d838-4d46-a2ae-be17eca49662",
   "metadata": {},
   "source": [
    "### **1. Language Filtering**\n",
    "\n",
    "* [✔️] Detect language of each post.\n",
    "* [✔️] Remove posts in Sinhala, Tamil, or any non-English languages.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Text Normalization**\n",
    "\n",
    "* [ ] Convert all text to lowercase.\n",
    "* [ ] Strip leading and trailing whitespaces.\n",
    "* [ ] Normalize unicode characters (optional, e.g., accented letters).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Removing Irrelevant Content**\n",
    "\n",
    "* [ ] Remove URLs (e.g., `http://`, `https://`, `www.`).\n",
    "* [ ] Remove emojis and emoticons.\n",
    "* [ ] Remove platform-specific metadata (e.g., `reddit`, `u/username`, `r/subreddit`, `comments`).\n",
    "* [ ] Remove escape sequences (e.g., `\\n`, `\\t`).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Handling Special Characters**\n",
    "\n",
    "* [ ] Remove punctuation marks (`!`, `.`, `,`, `?`, etc.).\n",
    "* [ ] Remove other non-alphanumeric characters (`@`, `#`, `%`, `^`, etc.).\n",
    "* [ ] Optionally remove numbers (if not needed for analysis).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Tokenization and Text Structuring**\n",
    "\n",
    "* [ ] Tokenize text into words (if needed for further analysis).\n",
    "* [ ] Optionally remove stopwords (`the`, `is`, `and`, etc.).\n",
    "* [ ] Optionally lemmatize or stem words.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Filtering by Length**\n",
    "\n",
    "* [ ] Remove posts with fewer than 25 words.\n",
    "* [ ] Remove posts with more than 1000 words.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Corpus Analysis**\n",
    "\n",
    "* [ ] Calculate the total number of words in the cleaned dataset.\n",
    "* [ ] Calculate the number of unique words in the cleaned dataset.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Optional / Advanced Steps**\n",
    "\n",
    "* [✔️] Handle duplicate posts if any.\n",
    "* [ ] Correct common typos or spelling errors.\n",
    "* [ ] Normalize spacing between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269d332-f121-47a5-826a-6d39a9fa12f5",
   "metadata": {},
   "source": [
    "# Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65b95f-5a8c-4d3f-9bdf-7bda2e0014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc3251-fc5e-4a61-bd44-b59c78f83070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3002fcd-372b-4b0a-b3f4-b0eb00c02a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace emojis with textual description\n",
    "def emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761b816-5a43-4f9f-8145-af51db83db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert emojis to text\n",
    "    text = emoji_to_text(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove Reddit platform metadata (u/username, r/subreddit)\n",
    "    text = re.sub(r'u\\/\\w+', '', text)\n",
    "    text = re.sub(r'r\\/\\w+', '', text)\n",
    "    \n",
    "    # Remove escape sequences and extra whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove punctuation (except within words like can't, won't)\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Join back into string\n",
    "    cleaned_text = \" \".join(lemmatized)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb036458-ecda-48d1-b53a-1d5414f508de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function\n",
    "df_posts['content_cleaned'] = df_posts['content'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02e9b7-e3e9-4f47-bb7e-f26e4a88cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter posts by word count by defining lower and upper treshold values to remove outliers in posts lengths\n",
    "# min_words = 25\n",
    "# max_words = 1000\n",
    "\n",
    "# def word_count_filter(text):\n",
    "#     count = len(text.split())\n",
    "#     return min_words <= count <= max_words\n",
    "\n",
    "# df_posts = df_posts[df_posts['content_cleaned'].apply(word_count_filter)]\n",
    "\n",
    "# Create a word count column\n",
    "df_posts[\"word_count\"] = df_posts[\"content_cleaned\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# 2. Plot histogram of word counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df_posts[\"word_count\"])\n",
    "plt.xlabel(\"Word Count per Document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Word Counts in df_posts['content_cleaned']\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Compute recommended thresholds using quantiles\n",
    "lower_threshold = df_posts[\"word_count\"].quantile(0.05)\n",
    "upper_threshold = df_posts[\"word_count\"].quantile(0.95)\n",
    "\n",
    "print(\"Lower Threshold (5th percentile):\", lower_threshold)\n",
    "print(\"Upper Threshold (95th percentile):\", upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ba0d7-7d88-410f-bfc9-69585628f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Report corpus statistics\n",
    "all_text = \" \".join(df_posts['content_cleaned'])\n",
    "all_words = all_text.split()\n",
    "total_words = len(all_words)\n",
    "unique_words = len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af03fd-3942-450f-b484-24ea6c93d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total words in cleaned corpus: {total_words}\")\n",
    "print(f\"Unique words in cleaned corpus: {unique_words}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a468c9-0e32-49e2-a16b-58193063c0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d17260-f5cd-4e21-8481-4f93369073fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c7e78-8876-4cb9-9dd6-3e1dccc7eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3364-07aa-48ea-8ecb-47017be5ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f78c3d-87c6-4462-8c99-5672afa68212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ebeac-28b9-4c21-a126-4420cd12afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode characters using NFKC.\n",
    "    Converts full-width characters, combined characters, and compatibility characters \n",
    "    into a consistent canonical form.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "# Apply normalization\n",
    "df_posts[\"content_cleaned\"] = (\n",
    "    df_posts[\"content_cleaned\"]\n",
    "    .astype(str)\n",
    "    .apply(normalize_unicode)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9235e7-7031-415e-b519-03b6cbd2bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2d644-226f-4eb3-a4b9-d7c9730f2501",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "## Traditional Word-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539858eb-f3a0-428b-bcc6-5605f4376337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "# -----------------------------\n",
    "# 2.1 Word-level tokenizer + lemmatization + stopwords removal\n",
    "# -----------------------------\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# -----------------------------\n",
    "# 2.2 Prepare corpus for SentencePiece\n",
    "# -----------------------------\n",
    "corpus_file = \"corpus.txt\"\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in df_posts[\"content_cleaned\"]:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2.3 Train BPE tokenizer\n",
    "# -----------------------------\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=bpe --vocab_size=8000 --model_type=bpe\"\n",
    ")\n",
    "bpe = spm.SentencePieceProcessor()\n",
    "bpe.load(\"bpe.model\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2.4 Train Unigram LM tokenizer\n",
    "# -----------------------------\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=unigram --vocab_size=8000 --model_type=unigram\"\n",
    ")\n",
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(\"unigram.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7bea7-fc9e-4897-9276-97c8df0390b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level tokenizer\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7154f20-a194-4e6d-9b98-60a8fb5668ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts[\"processed\"] = df_posts[\"content_cleaned\"].apply(word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50e6cd-d4b4-412f-958a-3fb10e781501",
   "metadata": {},
   "source": [
    "## Prepare corpus for SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee33c27-4779-4cc3-8ad3-3a0a616de735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for SentencePiece\n",
    "corpus_file = \"corpus.txt\"\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in df_posts[\"content_cleaned\"]:\n",
    "        f.write(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911460ac-7497-45cb-85f1-ed1c79d2e7e9",
   "metadata": {},
   "source": [
    "### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22435aa6-f316-4096-b262-d4cd2dd6f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=bpe --vocab_size=8000 --model_type=bpe\"\n",
    ")\n",
    "bpe = spm.SentencePieceProcessor()\n",
    "bpe.load(\"bpe.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218db90-4597-4643-a8be-fd8402cf3742",
   "metadata": {},
   "source": [
    "### Unigram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1309a67-1c85-4642-99bb-8a3d779e0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=unigram --vocab_size=8000 --model_type=unigram\"\n",
    ")\n",
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(\"unigram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efc7d6-2799-49b2-aa41-f98133f45dc1",
   "metadata": {},
   "source": [
    "## Compare Tokenization Schemes Impact using Corpus Statistics\n",
    "\n",
    "Calculating:\n",
    "- vocabulary size\n",
    "- average tokens per document\n",
    "- total tokens\n",
    "- rare token frequency (<3 occurrences)\n",
    "- OOV rate (for word-based tokenizers only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc559e0-ef35-44b1-a305-7b918ad63139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORPUS STATISTICS PER TOKENIZER\n",
    "\n",
    "def compute_stats(tokenizer_fn, tokenizer_name):\n",
    "    all_tokens = []\n",
    "    doc_lengths = []\n",
    "\n",
    "    for text in df_posts[\"content_cleaned\"]:\n",
    "        tokens = tokenizer_fn(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        doc_lengths.append(len(tokens))\n",
    "\n",
    "    vocab = set(all_tokens)\n",
    "    counter = collections.Counter(all_tokens)\n",
    "\n",
    "    return {\n",
    "        \"Tokenizer\": tokenizer_name,\n",
    "        \"Vocabulary Size\": len(vocab),\n",
    "        \"Total Tokens\": len(all_tokens),\n",
    "        \"Avg Tokens per Doc\": sum(doc_lengths)/len(doc_lengths),\n",
    "        \"Rare Tokens (<3)\": sum(1 for t,c in counter.items() if c < 3)\n",
    "    }\n",
    "\n",
    "stats_word = compute_stats(word_tokenizer, \"Word+Lemmatization\")\n",
    "stats_bpe = compute_stats(lambda t: bpe.encode(t, out_type=str), \"BPE\")\n",
    "stats_unigram = compute_stats(lambda t: unigram.encode(t, out_type=str), \"Unigram LM\")\n",
    "\n",
    "stats_df = pd.DataFrame([stats_word, stats_bpe, stats_unigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98091a65-06f3-4fdf-a1e1-12e30aa6d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee644717-5b74-4dfb-b83f-6920a2d0a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to Tokenizer (for easier plotting)\n",
    "plot_df = stats_df.set_index(\"Tokenizer\")\n",
    "\n",
    "# Plot each metric\n",
    "plot_df.plot(kind=\"bar\", figsize=(10,6))\n",
    "plt.title(\"Comparison of Tokenizer Statistics\")\n",
    "plt.xlabel(\"Tokenizer\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026f105-c7de-4e21-ad35-bc601c01d873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cdf74-333a-4f24-b4c3-e2af7dabf5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414332c-d7e4-46fd-9940-014f0662f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM LANGUAGE MODEL\n",
    "\n",
    "# -----------------------------\n",
    "# 4.1 Dataset class\n",
    "# -----------------------------\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, token_ids, seq_len=30):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_len])\n",
    "        y = torch.tensor(self.data[idx+1:idx+1+self.seq_len])\n",
    "        return x, y\n",
    "\n",
    "# -----------------------------\n",
    "# 4.2 LSTM LM model\n",
    "# -----------------------------\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        o, _ = self.lstm(e)\n",
    "        return self.fc(o)\n",
    "\n",
    "# -----------------------------\n",
    "# 4.3 Training function\n",
    "# -----------------------------\n",
    "def train_lm(model, dataloader, epochs=2):\n",
    "    model.train()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            optim.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# 4.4 Perplexity\n",
    "# -----------------------------\n",
    "def perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    total_loss, total_tokens = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += y.numel()\n",
    "\n",
    "    return math.exp(total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe1b9-d201-42ec-955c-04b0b4e6a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN LSTMs PER TOKENIZER\n",
    "\n",
    "SEQ, BATCH = 30, 32\n",
    "\n",
    "# -------- Word-level ----------\n",
    "word_vocab = {w:i for i,w in enumerate(set(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer)\n",
    ")))}\n",
    "word_id = lambda toks: [word_vocab[t] for t in toks if t in word_vocab]\n",
    "\n",
    "word_ids = list(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer).apply(word_id)\n",
    "))\n",
    "word_dl = DataLoader(LMDataset(word_ids, SEQ), batch_size=BATCH)\n",
    "lm_word = LSTMLM(len(word_vocab))\n",
    "lm_word = train_lm(lm_word, word_dl)\n",
    "pp_word = perplexity(lm_word, word_dl)\n",
    "\n",
    "# -------- BPE ----------\n",
    "bpe_ids = [i for t in df_posts[\"content_cleaned\"] for i in bpe.encode(t)]\n",
    "bpe_dl = DataLoader(LMDataset(bpe_ids, SEQ), batch_size=BATCH)\n",
    "lm_bpe = LSTMLM(bpe.get_piece_size())\n",
    "lm_bpe = train_lm(lm_bpe, bpe_dl)\n",
    "pp_bpe = perplexity(lm_bpe, bpe_dl)\n",
    "\n",
    "# -------- Unigram ----------\n",
    "uni_ids = [i for t in df_posts[\"content_cleaned\"] for i in unigram.encode(t)]\n",
    "uni_dl = DataLoader(LMDataset(uni_ids, SEQ), batch_size=BATCH)\n",
    "lm_uni = LSTMLM(unigram.get_piece_size())\n",
    "lm_uni = train_lm(lm_uni, uni_dl)\n",
    "pp_uni = perplexity(lm_uni, uni_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ab0b5-2234-455d-80e7-a9463431edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL COMPARISON TABLE\n",
    "\n",
    "final_results = pd.DataFrame({\n",
    "    \"Tokenizer\": [\"Word+Lemmatization\", \"BPE\", \"Unigram\"],\n",
    "    \"Vocabulary Size\": [stats_word[\"Vocabulary Size\"], stats_bpe[\"Vocabulary Size\"], stats_unigram[\"Vocabulary Size\"]],\n",
    "    \"Total Tokens\": [stats_word[\"Total Tokens\"], stats_bpe[\"Total Tokens\"], stats_unigram[\"Total Tokens\"]],\n",
    "    \"Avg Tokens per Doc\": [stats_word[\"Avg Tokens per Doc\"], stats_bpe[\"Avg Tokens per Doc\"], stats_unigram[\"Avg Tokens per Doc\"]],\n",
    "    \"Rare Tokens (<3)\": [stats_word[\"Rare Tokens (<3)\"], stats_bpe[\"Rare Tokens (<3)\"], stats_unigram[\"Rare Tokens (<3)\"]],\n",
    "    \"Perplexity\": [pp_word, pp_bpe, pp_uni]\n",
    "})\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ca153-cd81-4063-b7da-d8c76a47f659",
   "metadata": {},
   "source": [
    "## ✅ PIPELINE EXPLANATION\n",
    "\n",
    "Phase 1 — Cleaning\n",
    "\n",
    "Remove non-English posts\n",
    "\n",
    "Normalize unicode\n",
    "\n",
    "Remove URLs, emojis → text\n",
    "\n",
    "Remove Reddit metadata, punctuation, stopwords\n",
    "\n",
    "Lowercase & strip\n",
    "\n",
    "Filter posts <25 or >1000 words\n",
    "\n",
    "Phase 2 — Tokenization\n",
    "\n",
    "Word-level + lemmatization\n",
    "\n",
    "SentencePiece BPE\n",
    "\n",
    "SentencePiece Unigram\n",
    "\n",
    "Phase 3 — Corpus Statistics\n",
    "\n",
    "Compute vocab size, total tokens, avg tokens/doc, rare tokens\n",
    "\n",
    "Phase 4 — LSTM LM\n",
    "\n",
    "Build LSTM language model\n",
    "\n",
    "Train on each tokenization scheme\n",
    "\n",
    "Phase 5 — Train LSTM + Perplexity\n",
    "\n",
    "Compare model performance\n",
    "\n",
    "Phase 6 — Final Table\n",
    "\n",
    "Compare vocab, tokens, and perplexity\n",
    "\n",
    "Recommend best tokenizer (likely BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c44f5-69ed-4571-bb03-e152da3ffc92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
