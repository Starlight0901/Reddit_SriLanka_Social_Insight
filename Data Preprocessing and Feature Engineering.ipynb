{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097bf05-6241-43f3-934c-0ca02218e8f6",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672397e2-7d58-4142-a3b2-b5dd095428d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (2.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970ff0e2-6eae-4c89-a7f1-41e6072aa903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     -------- ------------------------------ 2.9/12.8 MB 830.6 kB/s eta 0:00:12\n",
      "     -------- ------------------------------ 2.9/12.8 MB 830.6 kB/s eta 0:00:12\n",
      "     -------- ------------------------------ 2.9/12.8 MB 830.6 kB/s eta 0:00:12\n",
      "     --------- ----------------------------- 3.1/12.8 MB 775.4 kB/s eta 0:00:13\n",
      "     --------- ----------------------------- 3.1/12.8 MB 775.4 kB/s eta 0:00:13\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 751.1 kB/s eta 0:00:13\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 751.1 kB/s eta 0:00:13\n",
      "     ----------- --------------------------- 3.7/12.8 MB 746.8 kB/s eta 0:00:13\n",
      "     ----------- --------------------------- 3.7/12.8 MB 746.8 kB/s eta 0:00:13\n",
      "     ----------- --------------------------- 3.9/12.8 MB 743.2 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 743.2 kB/s eta 0:00:12\n",
      "     ------------ -------------------------- 4.2/12.8 MB 740.1 kB/s eta 0:00:12\n",
      "     ------------- ------------------------- 4.5/12.8 MB 743.5 kB/s eta 0:00:12\n",
      "     -------------- ------------------------ 4.7/12.8 MB 756.5 kB/s eta 0:00:11\n",
      "     -------------- ------------------------ 4.7/12.8 MB 756.5 kB/s eta 0:00:11\n",
      "     --------------- ----------------------- 5.0/12.8 MB 768.4 kB/s eta 0:00:11\n",
      "     --------------- ----------------------- 5.2/12.8 MB 777.5 kB/s eta 0:00:10\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 791.4 kB/s eta 0:00:10\n",
      "     ----------------- --------------------- 5.8/12.8 MB 800.7 kB/s eta 0:00:09\n",
      "     ------------------ -------------------- 6.0/12.8 MB 807.6 kB/s eta 0:00:09\n",
      "     ------------------- ------------------- 6.3/12.8 MB 817.5 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.6/12.8 MB 831.9 kB/s eta 0:00:08\n",
      "     -------------------- ------------------ 6.8/12.8 MB 845.6 kB/s eta 0:00:08\n",
      "     --------------------- ----------------- 7.1/12.8 MB 857.0 kB/s eta 0:00:07\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 872.8 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.9/12.8 MB 897.6 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 910.1 kB/s eta 0:00:06\n",
      "     ------------------------- ------------- 8.4/12.8 MB 922.1 kB/s eta 0:00:05\n",
      "     -------------------------- ------------ 8.7/12.8 MB 932.1 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 8.9/12.8 MB 941.5 kB/s eta 0:00:05\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 967.4 kB/s eta 0:00:04\n",
      "     ----------------------------- --------- 9.7/12.8 MB 978.9 kB/s eta 0:00:04\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.0 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 10.5/12.8 MB 1.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 10.7/12.8 MB 1.0 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 1.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.8/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.0 MB/s  0:00:12\n",
      "Collecting spacy<3.8.0,>=3.7.2 (from en-core-web-sm==3.7.1)\n",
      "  Downloading spacy-3.7.5-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.3)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.11.12)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.3.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading marisa_trie-1.3.1-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Downloading spacy-3.7.5-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.1 MB 882.6 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.5/12.1 MB 882.6 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/12.1 MB 713.3 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.8/12.1 MB 713.3 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 1.0/12.1 MB 699.0 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 817.9 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 864.6 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.8/12.1 MB 898.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 1.8/12.1 MB 898.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.1 MB 903.1 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.1 MB 903.1 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.4/12.1 MB 843.9 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.4/12.1 MB 843.9 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.6/12.1 MB 811.6 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.9/12.1 MB 830.4 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.9/12.1 MB 830.4 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.9/12.1 MB 830.4 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 753.1 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 765.4 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 765.4 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.7/12.1 MB 759.8 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.7/12.1 MB 759.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 3.9/12.1 MB 762.5 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 4.2/12.1 MB 760.3 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 4.2/12.1 MB 760.3 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 4.5/12.1 MB 771.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 4.7/12.1 MB 781.3 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 794.6 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 807.0 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.1 MB 818.3 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.1 MB 818.3 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.1 MB 818.3 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.1 MB 818.3 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.1 MB 818.3 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.1 MB 749.6 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.1 MB 749.6 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 5.8/12.1 MB 749.6 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.1 MB 723.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.1 MB 723.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.1 MB 723.7 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.0/12.1 MB 723.7 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.3/12.1 MB 679.4 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.3/12.1 MB 679.4 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.3/12.1 MB 679.4 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.6/12.1 MB 653.7 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.6/12.1 MB 653.7 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.6/12.1 MB 653.7 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.6/12.1 MB 653.7 kB/s eta 0:00:09\n",
      "   --------------------- ------------------ 6.6/12.1 MB 653.7 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 625.0 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 625.0 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 625.0 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 605.0 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 605.0 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 605.0 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.3/12.1 MB 601.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 7.9/12.1 MB 629.4 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.1/12.1 MB 642.0 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.4/12.1 MB 655.8 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 678.5 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 9.2/12.1 MB 690.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 9.4/12.1 MB 701.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 10.0/12.1 MB 725.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.2/12.1 MB 735.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 758.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.1 MB 770.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 11.5/12.1 MB 796.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 806.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 806.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 806.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 806.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 780.6 kB/s  0:00:15\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 985.5 kB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.1 MB/s  0:00:01\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 1.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.3/6.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.4/6.6 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.6/6.6 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.1/6.6 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.2/6.6 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.5/6.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.0/6.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.0/6.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.2/6.6 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.0/6.6 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 1.4 MB/s  0:00:04\n",
      "Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/15.8 MB 1.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.0/15.8 MB 1.9 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.6/15.8 MB 2.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.8/15.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.4/15.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.6/15.8 MB 2.0 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 3.1/15.8 MB 1.9 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.4/15.8 MB 1.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.7/15.8 MB 1.8 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 4.2/15.8 MB 1.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 4.5/15.8 MB 1.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 4.7/15.8 MB 1.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 5.2/15.8 MB 1.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 5.5/15.8 MB 1.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 6.0/15.8 MB 1.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 6.3/15.8 MB 1.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.6/15.8 MB 1.8 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 7.1/15.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 7.3/15.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 7.9/15.8 MB 1.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 8.4/15.8 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.9/15.8 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.0/15.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.2/15.8 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 10.5/15.8 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.7/15.8 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.7/15.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 11.3/15.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.5/15.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.1/15.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.3/15.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 12.6/15.8 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 12.8/15.8 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.1/15.8 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.4/15.8 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.6/15.8 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.8 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 1.7 MB/s  0:00:09\n",
      "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 1.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.3/5.4 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.8/5.4 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.4/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 3.7/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 1.6 MB/s  0:00:03\n",
      "Downloading marisa_trie-1.3.1-cp310-cp310-win_amd64.whl (143 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: shellingham, numpy, mdurl, marisa-trie, markdown-it-py, language-data, blis, rich, langcodes, typer, thinc, spacy, en-core-web-sm\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.1.2\n",
      "\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "    Uninstalling numpy-2.1.2:\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   --- ------------------------------------  1/13 [numpy]\n",
      "   ------------ ---------------------------  4/13 [markdown-it-py]\n",
      "   ------------ ---------------------------  4/13 [markdown-it-py]\n",
      "   ------------ ---------------------------  4/13 [markdown-it-py]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "  Attempting uninstall: blis\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "    Found existing installation: blis 1.3.3\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "    Uninstalling blis-1.3.3:\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "      Successfully uninstalled blis-1.3.3\n",
      "   --------------- ------------------------  5/13 [language-data]\n",
      "   ------------------ ---------------------  6/13 [blis]\n",
      "   ------------------ ---------------------  6/13 [blis]\n",
      "   --------------------- ------------------  7/13 [rich]\n",
      "   --------------------- ------------------  7/13 [rich]\n",
      "   --------------------- ------------------  7/13 [rich]\n",
      "   --------------------- ------------------  7/13 [rich]\n",
      "   ------------------------ ---------------  8/13 [langcodes]\n",
      "   --------------------------- ------------  9/13 [typer]\n",
      "  Attempting uninstall: thinc\n",
      "   --------------------------- ------------  9/13 [typer]\n",
      "    Found existing installation: thinc 8.3.10\n",
      "   --------------------------- ------------  9/13 [typer]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "    Uninstalling thinc-8.3.10:\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "      Successfully uninstalled thinc-8.3.10\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "  Attempting uninstall: spacy\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "    Found existing installation: spacy 3.8.11\n",
      "   ------------------------------ --------- 10/13 [thinc]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "    Uninstalling spacy-3.8.11:\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "      Successfully uninstalled spacy-3.8.11\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "    Found existing installation: en_core_web_sm 3.8.0\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "    Uninstalling en_core_web_sm-3.8.0:\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "      Successfully uninstalled en_core_web_sm-3.8.0\n",
      "   --------------------------------- ------ 11/13 [spacy]\n",
      "   ------------------------------------ --- 12/13 [en-core-web-sm]\n",
      "   ---------------------------------------- 13/13 [en-core-web-sm]\n",
      "\n",
      "Successfully installed blis-0.7.11 en-core-web-sm-3.7.1 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 numpy-1.26.4 rich-14.2.0 shellingham-1.5.4 spacy-3.7.5 thinc-8.2.5 typer-0.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\tharu\\anaconda3\\envs\\torch\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'C:\\Users\\tharu\\anaconda3\\envs\\torch\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'C:\\Users\\tharu\\anaconda3\\envs\\torch\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\tharu\\anaconda3\\envs\\torch\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37ee78a-983f-40b7-8add-c947f48b6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentencepiece nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9c4bd2-1de9-402c-9a5b-6cd6f095d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f84da2-aabd-4299-9ee5-c1e85b96afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2421d06-697f-4030-9c14-a3de0ee36f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tharu\\anaconda3\\envs\\torch\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd7c-fc07-4994-8853-bcad1c6ed0ac",
   "metadata": {},
   "source": [
    "# Commit to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b657452-1d16-4c01-9ed3-e38dfd5dc90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Categorizing posts for classification.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Data Preprocessing and Feature Engineering.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 4f0f108] comparing tokenization schemes,\n",
      " 3 files changed, 139 insertions(+), 6547 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: RPC failed; HTTP 408 curl 22 The requested URL returned error: 408\n",
      "send-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: the remote end hung up unexpectedly\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"comparing tokenization schemes,\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260df94-6335-4131-b1c5-962708bb99d7",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7905cacb-0df9-4190-a3a6-31539a46e900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4060 Laptop GPU')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141bad06-47a7-4b41-bbfd-0e62b5c7de50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"C:\\Users\\tharu\\anaconda3\\envs\\torch\\python.exe\"\n  * The NumPy version is: \"2.2.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\__init__.py:24\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\multiarray.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\overrides.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inspect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[0;32m     12\u001b[0m ARRAY_FUNCTIONS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer, TfidfVectorizer\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\__init__.py:73\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     __check_build,\n\u001b[0;32m     71\u001b[0m     _distributor_init,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\base.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_repr_html\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     resample,\n\u001b[0;32m     19\u001b[0m     shuffle,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\scipy\\sparse\\__init__.py:300\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\scipy\\sparse\\_base.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Base class for sparse matrices\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0;32m      6\u001b[0m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[0;32m      7\u001b[0m                        matrix, validateaxis, getdtype)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[0;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misspmatrix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missparse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparray\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseWarning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseEfficiencyWarning\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\scipy\\sparse\\_sputils.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prod\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[0;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupcast\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misscalarlike\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misintlike\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misshape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missequence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misdense\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mismatrix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_sum_dtype\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbroadcast_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m supported_dtypes \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mbool_, np\u001b[38;5;241m.\u001b[39mbyte, np\u001b[38;5;241m.\u001b[39mubyte, np\u001b[38;5;241m.\u001b[39mshort, np\u001b[38;5;241m.\u001b[39mushort, np\u001b[38;5;241m.\u001b[39mintc,\n\u001b[0;32m     18\u001b[0m                     np\u001b[38;5;241m.\u001b[39muintc, np_long, np_ulong, np\u001b[38;5;241m.\u001b[39mlonglong, np\u001b[38;5;241m.\u001b[39mulonglong,\n\u001b[0;32m     19\u001b[0m                     np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mlongdouble,\n\u001b[0;32m     20\u001b[0m                     np\u001b[38;5;241m.\u001b[39mcomplex64, np\u001b[38;5;241m.\u001b[39mcomplex128, np\u001b[38;5;241m.\u001b[39mclongdouble]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\scipy\\_lib\\_util.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias, TypeVar\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n\u001b[0;32m     17\u001b[0m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\scipy\\_lib\\_array_api.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnpt\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     is_array_api_obj,\n\u001b[0;32m     20\u001b[0m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[0;32m     21\u001b[0m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[0;32m     22\u001b[0m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[0;32m     23\u001b[0m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[0;32m     24\u001b[0m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[0;32m     25\u001b[0m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[0;32m     26\u001b[0m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[0;32m     27\u001b[0m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_asarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_almost_equal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massert_array_almost_equal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_xp_devices\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxp_take_along_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxp_unsupported_param_msg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxp_vector_norm\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     39\u001b[0m ]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\scipy\\_lib\\array_api_compat\\numpy\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\__init__.py:355\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__dir__\u001b[39m():\n\u001b[0;32m    351\u001b[0m     public_symbols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m    352\u001b[0m     public_symbols \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatrixlib\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;66;03m# These were moved in 1.25 and may be deprecated eventually:\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisibleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplexWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTooHardError\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAxisError\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m     }\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(public_symbols)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\testing\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munittest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestCase\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extbuild\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msysconfig\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m isfinite, isnan, isinf\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_umath_linalg\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\__init__.py:50\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m], sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m1\u001b[39m], sys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[0;32m     49\u001b[0m         __version__, exc)\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m envkey \u001b[38;5;129;01min\u001b[39;00m env_added:\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"C:\\Users\\tharu\\anaconda3\\envs\\torch\\python.exe\"\n  * The NumPy version is: \"2.2.5\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sentencepiece as spm\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from langdetect import detect\n",
    "import langdetect\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy loaded OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a289-471b-440f-8e73-57d9c0c0120a",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f29ed-ddac-4873-b8f7-b54a183f4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc654212-751a-4707-ae63-580c3f5c958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset shape:\", df_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c4822-97df-4205-a720-40df51d9dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on a specific column\n",
    "duplicates_name = df_posts.duplicated(subset=['content'])\n",
    "print(\"\\nDuplicates based on 'content':\")\n",
    "print(df_posts[duplicates_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c3f2e-a6a3-4abf-9a83-44d8c0527e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e6205-4161-488b-95aa-fe05e2398424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55767c-f6b5-4d1b-a33d-1d52e211ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'keyword' missing values\n",
    "df_posts['keyword'] = df_posts['keyword'].fillna('no keyword')\n",
    "\n",
    "# Fill 'author' missing values\n",
    "df_posts['author'] = df_posts['author'].fillna('no author')\n",
    "\n",
    "# Fill 'score' missing values with the median\n",
    "median_score = df_posts['score'].median()\n",
    "df_posts['score'] = df_posts['score'].fillna(median_score)\n",
    "\n",
    "# Fill 'num_comments' missing values with 0 only where source == 'comments'\n",
    "mask = df_posts['source'] == 'comments'\n",
    "df_posts.loc[mask, 'num_comments'] = df_posts.loc[mask, 'num_comments'].fillna(0)\n",
    "\n",
    "# Fill 'parent_post' missing values\n",
    "df_posts['parent_post'] = df_posts['parent_post'].fillna('no post')\n",
    "\n",
    "# Drop rows where 'content' is missing (only 1 row)\n",
    "df_posts = df_posts.dropna(subset=['content'])\n",
    "# reset the index\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) \n",
    "\n",
    "print(\"\\n Dataset shape:\", df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bf730-1b88-45fc-8826-c2191464a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date column into a readable format\n",
    "df_posts['created_date'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.date\n",
    "df_posts['created_time'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.time\n",
    "\n",
    "# Drop the original 'created_utc' column\n",
    "df_posts.drop(columns=['created_utc'], inplace=True)\n",
    "df_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4e95-d80e-45a3-a350-8c9920010d27",
   "metadata": {},
   "source": [
    "### Drop non-english data (sinhala and tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f705a-fb07-4324-83f1-f3198e3ebb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Create a new column for language\n",
    "df_posts['language'] = df_posts['content'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0387c0-c623-44f6-bd16-9e0c725d21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only English posts\n",
    "df_posts = df_posts[df_posts['language'] == 'en'].copy()\n",
    "\n",
    "# Drop the language column\n",
    "df_posts.drop(columns=['language'], inplace=True)\n",
    "print(df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269d332-f121-47a5-826a-6d39a9fa12f5",
   "metadata": {},
   "source": [
    "# Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65b95f-5a8c-4d3f-9bdf-7bda2e0014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy with ONLY tokenizer + lemmatizer (VERY FAST)\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"textcat\"])\n",
    "# nlp.max_length = 2_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc3251-fc5e-4a61-bd44-b59c78f83070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3002fcd-372b-4b0a-b3f4-b0eb00c02a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace emojis with textual description\n",
    "def emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d9452-6edf-4961-8713-1f4b12c5ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \")) # replace emojis with textual description\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text) # remove URLs\n",
    "    text = re.sub(r'u\\/\\w+|r\\/\\w+', '', text) # remove Reddit metadata\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text) # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # normalize whitespace\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca673b-618d-4b0f-a6e5-1e9a94de7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_spacy(texts):\n",
    "    cleaned = []\n",
    "    \n",
    "    # Process in parallel \n",
    "    for doc in nlp.pipe(texts, batch_size=1000, n_process=4):\n",
    "        lemmas = [\n",
    "            token.lemma_\n",
    "            for token in doc\n",
    "            if token.is_alpha and not token.is_stop\n",
    "        ]\n",
    "        cleaned.append(\" \".join(lemmas))\n",
    "    \n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613ec0d-d418-442b-aa23-612a916cea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for all rows\n",
    "df_posts[\"content_cleaned\"] = df_posts[\"content\"].apply(clean_text)\n",
    "\n",
    "# fast spaCy batch processing\n",
    "df_posts[\"content_cleaned\"] = clean_text_spacy(df_posts[\"content_cleaned\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761b816-5a43-4f9f-8145-af51db83db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert emojis to text\n",
    "    text = emoji_to_text(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove Reddit platform metadata (u/username, r/subreddit)\n",
    "    text = re.sub(r'u\\/\\w+', '', text)\n",
    "    text = re.sub(r'r\\/\\w+', '', text)\n",
    "    \n",
    "    # Remove escape sequences and extra whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove punctuation (except within words like can't, won't)\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Join back into string\n",
    "    cleaned_text = \" \".join(lemmatized)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb036458-ecda-48d1-b53a-1d5414f508de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply cleaning function\n",
    "# df_posts['content_cleaned'] = df_posts['content'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02e9b7-e3e9-4f47-bb7e-f26e4a88cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word count column\n",
    "df_posts[\"word_count\"] = df_posts[\"content_cleaned\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot histogram of word counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df_posts[\"word_count\"])\n",
    "plt.xlabel(\"Word Count per Document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Word Counts in df_posts['content_cleaned']\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b487d6-7a7e-4f16-8a76-0c52afd72903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute recommended thresholds using quantiles\n",
    "lower_threshold = df_posts[\"word_count\"].quantile(0.3)\n",
    "upper_threshold = df_posts[\"word_count\"].quantile(0.95)\n",
    "\n",
    "print(\"Lower Threshold (10th percentile):\", lower_threshold)\n",
    "print(\"Upper Threshold (95th percentile):\", upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6499c39-7245-4f9d-9f56-7c490af00edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_backup = df_posts.copy()\n",
    "df_posts = df_posts[\n",
    "    (df_posts[\"word_count\"] >= lower_threshold) &\n",
    "    (df_posts[\"word_count\"] <= upper_threshold)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ba0d7-7d88-410f-bfc9-69585628f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Report corpus statistics\n",
    "all_text = \" \".join(df_posts['content_cleaned']) \n",
    "all_words = all_text.split()\n",
    "total_words = len(all_words)\n",
    "unique_words = len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af03fd-3942-450f-b484-24ea6c93d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total words in cleaned corpus: {total_words}\")\n",
    "print(f\"Unique words in cleaned corpus: {unique_words}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c7e78-8876-4cb9-9dd6-3e1dccc7eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3364-07aa-48ea-8ecb-47017be5ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ebeac-28b9-4c21-a126-4420cd12afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode characters using NFKC.\n",
    "    Converts full-width characters, combined characters, and compatibility characters \n",
    "    into a consistent canonical form.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "# Apply normalization\n",
    "df_posts[\"content_cleaned\"] = (\n",
    "    df_posts[\"content_cleaned\"]\n",
    "    .astype(str)\n",
    "    .apply(normalize_unicode)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9235e7-7031-415e-b519-03b6cbd2bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f9436-577f-4e9a-a4ff-10023c80b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "\n",
    "df_posts.to_csv(\"cleaned_Posts_Data.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a3204-8c0e-47bf-bd7c-b81f36be1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df_posts = pd.read_csv(\"cleaned_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2d644-226f-4eb3-a4b9-d7c9730f2501",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "## Traditional Word-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7bea7-fc9e-4897-9276-97c8df0390b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level tokenizer\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7154f20-a194-4e6d-9b98-60a8fb5668ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts[\"processed\"] = df_posts[\"content_cleaned\"].apply(word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50e6cd-d4b4-412f-958a-3fb10e781501",
   "metadata": {},
   "source": [
    "## Prepare corpus for SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee33c27-4779-4cc3-8ad3-3a0a616de735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for SentencePiece\n",
    "corpus_file = \"corpus.txt\"\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in df_posts[\"content_cleaned\"]:\n",
    "        f.write(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911460ac-7497-45cb-85f1-ed1c79d2e7e9",
   "metadata": {},
   "source": [
    "### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22435aa6-f316-4096-b262-d4cd2dd6f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=bpe --vocab_size=8000 --model_type=bpe\"\n",
    ")\n",
    "bpe = spm.SentencePieceProcessor()\n",
    "bpe.load(\"bpe.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218db90-4597-4643-a8be-fd8402cf3742",
   "metadata": {},
   "source": [
    "### Unigram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1309a67-1c85-4642-99bb-8a3d779e0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=unigram --vocab_size=8000 --model_type=unigram\"\n",
    ")\n",
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(\"unigram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efc7d6-2799-49b2-aa41-f98133f45dc1",
   "metadata": {},
   "source": [
    "## Compare Tokenization Schemes Impact using Corpus Statistics\n",
    "\n",
    "Calculating:\n",
    "- vocabulary size\n",
    "- average tokens per document\n",
    "- total tokens\n",
    "- rare token frequency (<3 occurrences)\n",
    "- OOV rate (for word-based tokenizers only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc559e0-ef35-44b1-a305-7b918ad63139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus statistics per tokenizer\n",
    "\n",
    "def compute_stats(tokenizer_fn, tokenizer_name):\n",
    "    all_tokens = []\n",
    "    doc_lengths = []\n",
    "\n",
    "    for text in df_posts[\"content_cleaned\"]:\n",
    "        tokens = tokenizer_fn(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        doc_lengths.append(len(tokens))\n",
    "\n",
    "    vocab = set(all_tokens)\n",
    "    counter = collections.Counter(all_tokens)\n",
    "\n",
    "    return {\n",
    "        \"Tokenizer\": tokenizer_name,\n",
    "        \"Vocabulary Size\": len(vocab),\n",
    "        \"Total Tokens\": len(all_tokens),\n",
    "        \"Avg Tokens per Doc\": sum(doc_lengths)/len(doc_lengths),\n",
    "        \"Rare Tokens (<3)\": sum(1 for t,c in counter.items() if c < 3)\n",
    "    }\n",
    "\n",
    "stats_word = compute_stats(word_tokenizer, \"Word+Lemmatization\")\n",
    "stats_bpe = compute_stats(lambda t: bpe.encode(t, out_type=str), \"BPE\")\n",
    "stats_unigram = compute_stats(lambda t: unigram.encode(t, out_type=str), \"Unigram LM\")\n",
    "\n",
    "stats_df = pd.DataFrame([stats_word, stats_bpe, stats_unigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98091a65-06f3-4fdf-a1e1-12e30aa6d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee644717-5b74-4dfb-b83f-6920a2d0a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to Tokenizer (for easier plotting)\n",
    "plot_df = stats_df.set_index(\"Tokenizer\")\n",
    "\n",
    "# Plot each metric\n",
    "plot_df.plot(kind=\"bar\", figsize=(10,6))\n",
    "plt.title(\"Comparison of Tokenizer Statistics\")\n",
    "plt.xlabel(\"Tokenizer\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef04a8f-a35d-4f63-a1a6-ff2c18d2c02c",
   "metadata": {},
   "source": [
    "The tokenization analysis shows clear differences between the three approaches: Word+Lemmatization, BPE, and Unigram LMand the results strongly align with theoretical expectations. The word-level tokenizer produced an extremely large vocabulary of 47,222 items and a very high number of rare tokens (31,098), even after lemmatization and stopword removal. This indicates severe sparsity: each spelling variant, slang term, and morphological form becomes a separate token, making the representation unstable and prone to poor generalization. In contrast, BPE reduced the vocabulary size to 8,887 and dropped rare tokens to just 1,023, demonstrating its ability to decompose infrequent or noisy words into reusable subword units. This leads to a more robust and consistent representation, though at the cost of slightly longer token sequences, which is expected for subword models. The Unigram LM tokenizer achieved similar vocabulary compression (8,962 tokens) but yielded the lowest number of rare tokens (855), reflecting its probabilistic approach to selecting the most efficient and informative subword units. Overall, the results validate the progression predicted by NLP literature: word-level tokenization is the least efficient, BPE offers substantial improvements, and Unigram LM provides the most balanced and linguistically consistent tokenization strategy for noisy text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44a405-97c1-4b21-9468-7cbccc5bb2ba",
   "metadata": {},
   "source": [
    "# Evaluate Tokenization Schemes and effectiveness for LLM Tasks\n",
    "\n",
    "## LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09580281-328b-4ede-ae4f-06f7b61bede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU Training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414332c-d7e4-46fd-9940-014f0662f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, token_ids, seq_len=30):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+1+self.seq_len], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646096c3-06e9-4513-9a34-86333b9b8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM LM model\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        o, _ = self.lstm(e)\n",
    "        return self.fc(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaba90d-1f53-4b0f-8f5d-31ff677eeb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_lm(model, dataloader, epochs=3, lr=0.001):\n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optim.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0044e-71b7-4782-8afd-dcfdfd9f45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "def perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item() * x.numel()\n",
    "            total_tokens += x.numel()\n",
    "    return torch.exp(torch.tensor(total_loss / total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91890ad-e79e-456e-a8d0-36ba1e2e1ae6",
   "metadata": {},
   "source": [
    "### Train LSTMs per Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe1b9-d201-42ec-955c-04b0b4e6a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "SEQ = 30\n",
    "BATCH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36eecd-7cac-4b7b-82b2-07df6650cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level Tokenizer\n",
    "word_vocab = {w:i for i,w in enumerate(set(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer)\n",
    ")))}\n",
    "word_id = lambda toks: [word_vocab[t] for t in toks if t in word_vocab]\n",
    "\n",
    "word_ids = list(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer).apply(word_id)\n",
    "))\n",
    "word_dl = DataLoader(LMDataset(word_ids, SEQ), batch_size=BATCH)\n",
    "lm_word = LSTMLM(len(word_vocab)).to(device)\n",
    "lm_word = train_lm(lm_word, word_dl)\n",
    "pp_word = perplexity(lm_word, word_dl)\n",
    "print(\"Word-level LM perplexity:\", pp_word.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d62b70-6322-4c02-8cca-f1d46c30da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE\n",
    "bpe_ids = [i for t in df_posts[\"content_cleaned\"] for i in bpe.encode(t)]\n",
    "bpe_dl = DataLoader(LMDataset(bpe_ids, SEQ), batch_size=BATCH)\n",
    "lm_bpe = LSTMLM(bpe.get_piece_size()).to(device)\n",
    "lm_bpe = train_lm(lm_bpe, bpe_dl)\n",
    "pp_bpe = perplexity(lm_bpe, bpe_dl)\n",
    "print(\"BPE LM perplexity:\", pp_bpe.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8cef4-9e18-4902-a76e-421cfd7e8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram\n",
    "uni_ids = [i for t in df_posts[\"content_cleaned\"] for i in unigram.encode(t)]\n",
    "uni_dl = DataLoader(LMDataset(uni_ids, SEQ), batch_size=BATCH)\n",
    "lm_uni = LSTMLM(unigram.get_piece_size()).to(device)\n",
    "lm_uni = train_lm(lm_uni, uni_dl) \n",
    "pp_uni = perplexity(lm_uni, uni_dl)\n",
    "print(\"Unigram LM perplexity:\", pp_uni.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062470c-1250-4db1-9436-702e6239d5fc",
   "metadata": {},
   "source": [
    "### Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ab0b5-2234-455d-80e7-a9463431edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame({\n",
    "    \"Tokenizer\": [\"Word+Lemmatization\", \"BPE\", \"Unigram\"],\n",
    "    \"Vocabulary Size\": [stats_word[\"Vocabulary Size\"], stats_bpe[\"Vocabulary Size\"], stats_unigram[\"Vocabulary Size\"]],\n",
    "    \"Total Tokens\": [stats_word[\"Total Tokens\"], stats_bpe[\"Total Tokens\"], stats_unigram[\"Total Tokens\"]],\n",
    "    \"Avg Tokens per Doc\": [stats_word[\"Avg Tokens per Doc\"], stats_bpe[\"Avg Tokens per Doc\"], stats_unigram[\"Avg Tokens per Doc\"]],\n",
    "    \"Rare Tokens (<3)\": [stats_word[\"Rare Tokens (<3)\"], stats_bpe[\"Rare Tokens (<3)\"], stats_unigram[\"Rare Tokens (<3)\"]],\n",
    "    \"Perplexity\": [pp_word, pp_bpe, pp_uni]\n",
    "})\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37271053-ba84-4e6a-b42d-c8b07aab7474",
   "metadata": {},
   "source": [
    "The LSTM results clearly demonstrate how strongly recurrent models depend on the quality of tokenization. When trained on the word-level tokenizer, the LSTM produced the highest perplexity (884), largely because this tokenizer generated an extremely large vocabulary (over 36,000 unique words) and a very high proportion of rare terms. Such sparsity makes it difficult for the LSTM to learn stable embeddings or reliably predict the next token, since many words appear too infrequently for meaningful parameter updates. In contrast, both subword tokenizers (BPE and Unigram) dramatically reduced perplexityapproximately halving itbecause they compress the vocabulary into a smaller, denser, and more frequent token space. This reduction in sparsity enables the LSTM to learn more generalizable patterns such as common stems, prefixes, and suffixes, improving its ability to model text sequences. BPE achieved the lowest perplexity (479), slightly outperforming Unigram, likely because BPE produces more deterministic and consistent subword units that better suit the LSTMs sequential prediction process. Overall, the results show that subword tokenization substantially enhances LSTM performance by providing a more compact and informative input representation, while word-level tokenization overwhelms the model and leads to significantly poorer language modeling ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531afe05-8881-4638-acc8-ae98b851343c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
