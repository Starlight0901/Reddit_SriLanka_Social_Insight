{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097bf05-6241-43f3-934c-0ca02218e8f6",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672397e2-7d58-4142-a3b2-b5dd095428d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970ff0e2-6eae-4c89-a7f1-41e6072aa903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n",
      "     - 0 bytes ? 0:00:00\n",
      "     - 0 bytes ? 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Wheel 'en-core-web-sm' located at C:\\Users\\tharu\\AppData\\Local\\Temp\\pip-unpack-aydwv99q\\en_core_web_sm-3.7.1-py3-none-any.whl is invalid.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37ee78a-983f-40b7-8add-c947f48b6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentencepiece nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9c4bd2-1de9-402c-9a5b-6cd6f095d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f84da2-aabd-4299-9ee5-c1e85b96afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2421d06-697f-4030-9c14-a3de0ee36f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b026bedd-b88e-4695-9c39-09d973984388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from wordcloud) (11.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from wordcloud) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from matplotlib->wordcloud) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from matplotlib->wordcloud) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from matplotlib->wordcloud) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from matplotlib->wordcloud) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc88427-1a48-4bf7-ae4a-9f029607291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk>=3.9->textblob) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a14296b-9380-440c-b74d-368a844bd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (0.1.73)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\tharu\\anaconda3\\envs\\gpu-env\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd7c-fc07-4994-8853-bcad1c6ed0ac",
   "metadata": {},
   "source": [
    "# Commit to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b657452-1d16-4c01-9ed3-e38dfd5dc90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 10 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Data Preprocessing and Feature Engineering.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main afa3350] finetuning preprocesssing pipeline,\n",
      " 1 file changed, 2032 insertions(+), 139 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: RPC failed; HTTP 408 curl 22 The requested URL returned error: 408\n",
      "send-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: the remote end hung up unexpectedly\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"finetuning preprocesssing pipeline,\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260df94-6335-4131-b1c5-962708bb99d7",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7905cacb-0df9-4190-a3a6-31539a46e900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4060 Laptop GPU')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "141bad06-47a7-4b41-bbfd-0e62b5c7de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy loaded OK!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sentencepiece as spm\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy loaded OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a289-471b-440f-8e73-57d9c0c0120a",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237f29ed-ddac-4873-b8f7-b54a183f4f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_12216\\3966237626.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source keyword       id   created_utc             author subreddit  \\\n",
       "0  post    new     NaN  1otaemb  1.762771e+09     Cookiehere6969  srilanka   \n",
       "1  post    new     NaN  1otaam5  1.762770e+09           oshan789  srilanka   \n",
       "2  post    new     NaN  1ot9w1v  1.762769e+09          mgssjjsks  srilanka   \n",
       "3  post    new     NaN  1ot9kwe  1.762768e+09  Critical_Rise_exe  srilanka   \n",
       "4  post    new     NaN  1ot9h2f  1.762767e+09       No-Leave8971  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                  Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "\n",
       "  parent_post  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc654212-751a-4707-ae63-580c3f5c958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70861, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset shape:\", df_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "115c4822-97df-4205-a720-40df51d9dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates based on 'content':\n",
      "Empty DataFrame\n",
      "Columns: [type, source, keyword, id, created_utc, author, subreddit, content, score, num_comments, parent_post]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on a specific column\n",
    "duplicates_name = df_posts.duplicated(subset=['content'])\n",
    "print(\"\\nDuplicates based on 'content':\")\n",
    "print(df_posts[duplicates_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f8c3f2e-a6a3-4abf-9a83-44d8c0527e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "971e6205-4161-488b-95aa-fe05e2398424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc55767c-f6b5-4d1b-a33d-1d52e211ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword             0\n",
      "id                  0\n",
      "created_utc         0\n",
      "author              0\n",
      "subreddit           0\n",
      "content             0\n",
      "score               0\n",
      "num_comments    52020\n",
      "parent_post         0\n",
      "dtype: int64\n",
      "\n",
      " Dataset shape: (70860, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id   created_utc             author  \\\n",
       "0  post    new  no keyword  1otaemb  1.762771e+09     Cookiehere6969   \n",
       "1  post    new  no keyword  1otaam5  1.762770e+09           oshan789   \n",
       "2  post    new  no keyword  1ot9w1v  1.762769e+09          mgssjjsks   \n",
       "3  post    new  no keyword  1ot9kwe  1.762768e+09  Critical_Rise_exe   \n",
       "4  post    new  no keyword  1ot9h2f  1.762767e+09       No-Leave8971   \n",
       "\n",
       "  subreddit                                            content  score  \\\n",
       "0  srilanka  Is this a Scam or good investment? Haritha Lan...    2.0   \n",
       "1  srilanka  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0   \n",
       "2  srilanka  Whats your hot take on Sri Lanka as the title ...    3.0   \n",
       "3  srilanka                  Is the rs.11 deals real in Daraz?    1.0   \n",
       "4  srilanka  Need advice from the experts üôè [](https://www....    2.0   \n",
       "\n",
       "   num_comments parent_post  \n",
       "0           1.0     no post  \n",
       "1           0.0     no post  \n",
       "2           8.0     no post  \n",
       "3           3.0     no post  \n",
       "4           0.0     no post  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill 'keyword' missing values\n",
    "df_posts['keyword'] = df_posts['keyword'].fillna('no keyword')\n",
    "\n",
    "# Fill 'author' missing values\n",
    "df_posts['author'] = df_posts['author'].fillna('no author')\n",
    "\n",
    "# Fill 'score' missing values with the median\n",
    "median_score = df_posts['score'].median()\n",
    "df_posts['score'] = df_posts['score'].fillna(median_score)\n",
    "\n",
    "# Fill 'num_comments' missing values with 0 only where source == 'comments'\n",
    "mask = df_posts['source'] == 'comments'\n",
    "df_posts.loc[mask, 'num_comments'] = df_posts.loc[mask, 'num_comments'].fillna(0)\n",
    "\n",
    "# Fill 'parent_post' missing values\n",
    "df_posts['parent_post'] = df_posts['parent_post'].fillna('no post')\n",
    "\n",
    "# Drop rows where 'content' is missing (only 1 row)\n",
    "df_posts = df_posts.dropna(subset=['content'])\n",
    "# reset the index\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) \n",
    "\n",
    "print(\"\\n Dataset shape:\", df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b9bf730-1b88-45fc-8826-c2191464a088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:40:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70855</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozebo7</td>\n",
       "      <td>Dhanagg</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>NEWSWIRE\\n\\nSri Lanka flags\\noutside Rawalpind...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:55:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70856</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozdn89</td>\n",
       "      <td>Unreal_realist-7381</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>DOT STUDIOS PRESENTS\\nPASAN DOMINIC HASALAKA T...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70857</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozdi55</td>\n",
       "      <td>wiknew1</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>sarasavi fi Q\\n\\nTHE BOOKSHOP\\nOL LIST SARASAV...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:07:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70858</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozbed2</td>\n",
       "      <td>smllcheeseburger</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Lamborghini Urus Twin turbo V8 2025\\n\\nPosted ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>08:54:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70859</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1oz8366</td>\n",
       "      <td>Crimson_roses154</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Lid ao ol on oe at jas\\nPTAA UO ¬©</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>05:28:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70860 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           type source     keyword       id               author subreddit  \\\n",
       "0          post    new  no keyword  1otaemb       Cookiehere6969  srilanka   \n",
       "1          post    new  no keyword  1otaam5             oshan789  srilanka   \n",
       "2          post    new  no keyword  1ot9w1v            mgssjjsks  srilanka   \n",
       "3          post    new  no keyword  1ot9kwe    Critical_Rise_exe  srilanka   \n",
       "4          post    new  no keyword  1ot9h2f         No-Leave8971  srilanka   \n",
       "...         ...    ...         ...      ...                  ...       ...   \n",
       "70855  img_post    url  no keyword  1ozebo7              Dhanagg  srilanka   \n",
       "70856  img_post    url  no keyword  1ozdn89  Unreal_realist-7381  srilanka   \n",
       "70857  img_post    url  no keyword  1ozdi55              wiknew1  srilanka   \n",
       "70858  img_post    url  no keyword  1ozbed2     smllcheeseburger  srilanka   \n",
       "70859  img_post    url  no keyword  1oz8366     Crimson_roses154  srilanka   \n",
       "\n",
       "                                                 content  score  num_comments  \\\n",
       "0      Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1      Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2      Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                      Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4      Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "...                                                  ...    ...           ...   \n",
       "70855  NEWSWIRE\\n\\nSri Lanka flags\\noutside Rawalpind...    3.0           NaN   \n",
       "70856  DOT STUDIOS PRESENTS\\nPASAN DOMINIC HASALAKA T...    3.0           NaN   \n",
       "70857  sarasavi fi Q\\n\\nTHE BOOKSHOP\\nOL LIST SARASAV...    3.0           NaN   \n",
       "70858  Lamborghini Urus Twin turbo V8 2025\\n\\nPosted ...    3.0           NaN   \n",
       "70859                  Lid ao ol on oe at jas\\nPTAA UO ¬©    3.0           NaN   \n",
       "\n",
       "      parent_post created_date created_time  \n",
       "0         no post   2025-11-10     10:33:16  \n",
       "1         no post   2025-11-10     10:26:02  \n",
       "2         no post   2025-11-10     10:00:29  \n",
       "3         no post   2025-11-10     09:40:57  \n",
       "4         no post   2025-11-10     09:33:57  \n",
       "...           ...          ...          ...  \n",
       "70855     no post   2025-11-17     11:55:19  \n",
       "70856     no post   2025-11-17     11:16:00  \n",
       "70857     no post   2025-11-17     11:07:48  \n",
       "70858     no post   2025-11-17     08:54:42  \n",
       "70859     no post   2025-11-17     05:28:02  \n",
       "\n",
       "[70860 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting date column into a readable format\n",
    "df_posts['created_date'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.date\n",
    "df_posts['created_time'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.time\n",
    "\n",
    "# Drop the original 'created_utc' column\n",
    "df_posts.drop(columns=['created_utc'], inplace=True)\n",
    "df_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4e95-d80e-45a3-a350-8c9920010d27",
   "metadata": {},
   "source": [
    "### Drop non-english data (sinhala and tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f705a-fb07-4324-83f1-f3198e3ebb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Create a new column for language\n",
    "df_posts['language'] = df_posts['content'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586bb41-cb5f-47af-9f88-2042d7d2850a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0387c0-c623-44f6-bd16-9e0c725d21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only English posts\n",
    "df_posts = df_posts[df_posts['language'] == 'en'].copy()\n",
    "\n",
    "# Drop the language column\n",
    "df_posts.drop(columns=['language'], inplace=True)\n",
    "print(df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269d332-f121-47a5-826a-6d39a9fa12f5",
   "metadata": {},
   "source": [
    "# Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f69bb-d0f2-4560-9f38-01e96eb88f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import emoji\n",
    "# import unicodedata\n",
    "# import contractions\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # Mapping emojis ‚Üí sentiment categories (you can expand this)\n",
    "# emoji_sentiment_map = {\n",
    "#     \"joy\": \"EMO_POS\",\n",
    "#     \"grin\": \"EMO_POS\",\n",
    "#     \"smile\": \"EMO_POS\",\n",
    "#     \"heart\": \"EMO_POS\",\n",
    "#     \"sob\": \"EMO_NEG\",\n",
    "#     \"cry\": \"EMO_NEG\",\n",
    "#     \"angry\": \"EMO_NEG\",\n",
    "#     \"rage\": \"EMO_NEG\",\n",
    "# }\n",
    "# def map_emoji_to_sentiment(word):\n",
    "#     for key, val in emoji_sentiment_map.items():\n",
    "#         if key in word:\n",
    "#             return val\n",
    "#     return word\n",
    "\n",
    "\n",
    "# def clean_text(text, remove_stopwords=False, lemmatize=False):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "\n",
    "#     # Normalize Unicode + lowercase\n",
    "#     text = unicodedata.normalize(\"NFKC\", text.lower())\n",
    "\n",
    "#     # Expand contractions\n",
    "#     text = contractions.fix(text)\n",
    "\n",
    "#     # Replace emojis with words ‚Üí then map to positive/negative sentiment tokens\n",
    "#     text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "#     text = \" \".join(map_emoji_to_sentiment(w) for w in text.split())\n",
    "\n",
    "#     # Remove URLs + Reddit patterns\n",
    "#     text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "#     text = re.sub(r\"u\\/\\w+|r\\/\\w+\", \"\", text)\n",
    "\n",
    "#     # Remove filler words\n",
    "#     text = re.sub(r\"\\b(lol|haha+|hah+|omg|uh+|umm+|hmm+)\\b\", \" \", text)\n",
    "\n",
    "#     # Remove numbers\n",
    "#     text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "#     # Remove punctuation (keep apostrophes for now)\n",
    "#     text = re.sub(r\"[^\\w\\s']\", \" \", text)\n",
    "\n",
    "#     # Normalize repeated characters (\"soooo\" ‚Üí \"soo\")\n",
    "#     text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "#     # Handle negations (\"not good\" ‚Üí \"not_good\")\n",
    "#     text = re.sub(r\"\\bnot\\s+(\\w+)\", r\"not_\\1\", text)\n",
    "\n",
    "#     # Remove extra apostrophes\n",
    "#     text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "#     # Remove stopwords\n",
    "#     if remove_stopwords:\n",
    "#         text = \" \".join(w for w in text.split() if w not in stop_words)\n",
    "\n",
    "#     # Lemmatization\n",
    "#     if lemmatize:\n",
    "#         text = \" \".join(lemmatizer.lemmatize(w) for w in text.split())\n",
    "\n",
    "#     # Final whitespace cleanup\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "#     return text\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import contractions\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, sent_tokenize, word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Mapping emojis ‚Üí sentiment categories\n",
    "emoji_sentiment_map = {\n",
    "    \"joy\": \"EMO_POS\",\n",
    "    \"grin\": \"EMO_POS\",\n",
    "    \"smile\": \"EMO_POS\",\n",
    "    \"heart\": \"EMO_POS\",\n",
    "    \"sob\": \"EMO_NEG\",\n",
    "    \"cry\": \"EMO_NEG\",\n",
    "    \"angry\": \"EMO_NEG\",\n",
    "    \"rage\": \"EMO_NEG\",\n",
    "}\n",
    "\n",
    "def map_emoji_to_sentiment(word):\n",
    "    for key, val in emoji_sentiment_map.items():\n",
    "        if key in word:\n",
    "            return val\n",
    "    return word\n",
    "\n",
    "# Map NLTK POS tag ‚Üí WordNet POS\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(text, remove_stopwords=False, lemmatize=False):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize Unicode + lowercase\n",
    "    text = unicodedata.normalize(\"NFKC\", text.lower())\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Replace emojis with words ‚Üí then map to positive/negative sentiment tokens\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = \" \".join(map_emoji_to_sentiment(w) for w in text.split())\n",
    "\n",
    "    # Remove URLs + Reddit patterns\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"u\\/\\w+|r\\/\\w+\", \"\", text)\n",
    "\n",
    "    # Remove filler words\n",
    "    text = re.sub(r\"\\b(lol|haha+|hah+|omg|uh+|umm+|hmm+)\\b\", \" \", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "    # Remove punctuation (keep apostrophes for now)\n",
    "    text = re.sub(r\"[^\\w\\s']\", \" \", text)\n",
    "\n",
    "    # Normalize repeated characters (\"soooo\" ‚Üí \"soo\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "    # Handle negations (\"not good\" ‚Üí \"not_good\")\n",
    "    text = re.sub(r\"\\bnot\\s+(\\w+)\", r\"not_\\1\", text)\n",
    "\n",
    "    # Remove extra apostrophes\n",
    "    text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    processed_words = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "\n",
    "        # Remove stopwords if requested\n",
    "        if remove_stopwords:\n",
    "            words = [w for w in words if w not in stop_words]\n",
    "\n",
    "        # POS tagging and lemmatization\n",
    "        if lemmatize:\n",
    "            tags = pos_tag(words)\n",
    "            words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tags]\n",
    "\n",
    "        processed_words.extend(words)\n",
    "\n",
    "    # Join back into single string and clean extra spaces\n",
    "    text = \" \".join(processed_words)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783306e2-e4c7-4542-bb26-850850e80051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_posts[\"content_cleaned\"] = df_posts[\"content\"].apply(lambda x: clean_text(x, remove_stopwords=True, lemmatize=True))\n",
    "df_posts[\"content_cleaned\"] = df_posts[\"content\"].apply(lambda x: clean_text(x, remove_stopwords=True, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca1665-f01d-41ce-973e-384308b22d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47334508-2118-4d9b-b20d-552002659b03",
   "metadata": {},
   "source": [
    "## Check preprocessing \n",
    "\n",
    "### Check Lemmatization and POS Tagging Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4ad1b-8f94-491e-9436-51269fc5d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "test_words = [\"running\", \"ran\", \"cars\", \"better\", \"studies\", \"mice\"]\n",
    "tags = pos_tag(test_words)\n",
    "print(tags)\n",
    "\n",
    "sentence = \"I am running. He ran. The mice are fast.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b000dd-980d-458c-9081-4fc4eec2cbdc",
   "metadata": {},
   "source": [
    "### Check Distribution of Tokens Before/After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26803354-b4e3-44dd-ba68-05f2fa7f4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_vocab = set(\" \".join(df_posts[\"content\"]).split())\n",
    "clean_vocab = set(\" \".join(df_posts[\"content_cleaned\"]).split())\n",
    "\n",
    "print(\"Original vocab:\", len(orig_vocab))\n",
    "print(\"Clean vocab:\", len(clean_vocab))\n",
    "print(\"Reduction:\", len(orig_vocab)-len(clean_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb8b67-457e-4250-aab6-5409fca6e00a",
   "metadata": {},
   "source": [
    "### Check Word Frequency Before/After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031e922-d089-448c-9900-44d063e275fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "orig_counts = Counter(\" \".join(df_posts[\"content\"]).split()).most_common(20)\n",
    "clean_counts = Counter(\" \".join(df_posts[\"content_cleaned\"]).split()).most_common(20)\n",
    "\n",
    "print(\"\\nOriginal Top Words:\\n\", orig_counts)\n",
    "print(\"\\nCleaned Top Words:\\n\", clean_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7cbd0-39f2-4d8b-a43d-7baafa6f7a0f",
   "metadata": {},
   "source": [
    "### Plot most common words (top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe9874-1fb4-495a-b884-b029ec8c06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(series, top_n=20):\n",
    "    all_words = \" \".join(series).split()\n",
    "    counter = Counter(all_words)\n",
    "    return counter.most_common(top_n)\n",
    "\n",
    "top_words = get_top_words(df_posts[\"content_cleaned\"], top_n=20)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2756e-6886-4d08-b793-c62e9f879ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=list(counts),\n",
    "    y=list(words),\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "\n",
    "plt.title(\"Top 20 Most Frequent Words in Cleaned Posts\", fontsize=16)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82140c6-9056-40c5-9cbb-0c4c3baba2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb036458-ecda-48d1-b53a-1d5414f508de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply cleaning function\n",
    "# df_posts['content_cleaned'] = df_posts['content'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02e9b7-e3e9-4f47-bb7e-f26e4a88cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word count column\n",
    "df_posts[\"word_count\"] = df_posts[\"content_cleaned\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot histogram of word counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df_posts[\"word_count\"])\n",
    "plt.xlabel(\"Word Count per Document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Word Counts in df_posts['content_cleaned']\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b487d6-7a7e-4f16-8a76-0c52afd72903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute recommended thresholds using quantiles\n",
    "lower_threshold = df_posts[\"word_count\"].quantile(0.45)\n",
    "upper_threshold = df_posts[\"word_count\"].quantile(0.98)\n",
    "\n",
    "print(\"Lower Threshold (11th percentile):\", lower_threshold)\n",
    "print(\"Upper Threshold (95th percentile):\", upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6499c39-7245-4f9d-9f56-7c490af00edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_backup = df_posts.copy()\n",
    "df_posts = df_posts[\n",
    "    (df_posts[\"word_count\"] >= lower_threshold) &\n",
    "    (df_posts[\"word_count\"] <= upper_threshold)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133de0d-f74a-43b9-a62a-e46dde2a7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word count column\n",
    "df_posts[\"word_count\"] = df_posts[\"content_cleaned\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot histogram of word counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df_posts[\"word_count\"])\n",
    "plt.xlabel(\"Word Count per Document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Word Counts in df_posts['content_cleaned']\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ba0d7-7d88-410f-bfc9-69585628f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Report corpus statistics\n",
    "all_text = \" \".join(df_posts['content_cleaned']) \n",
    "all_words = all_text.split()\n",
    "total_words = len(all_words)\n",
    "unique_words = len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af03fd-3942-450f-b484-24ea6c93d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total words in cleaned corpus: {total_words}\")\n",
    "print(f\"Unique words in cleaned corpus: {unique_words}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c7e78-8876-4cb9-9dd6-3e1dccc7eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3364-07aa-48ea-8ecb-47017be5ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ebeac-28b9-4c21-a126-4420cd12afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode characters using NFKC.\n",
    "    Converts full-width characters, combined characters, and compatibility characters \n",
    "    into a consistent canonical form.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "# Apply normalization\n",
    "df_posts[\"content_cleaned\"] = (\n",
    "    df_posts[\"content_cleaned\"]\n",
    "    .astype(str)\n",
    "    .apply(normalize_unicode)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9235e7-7031-415e-b519-03b6cbd2bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f9436-577f-4e9a-a4ff-10023c80b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "\n",
    "df_posts.to_csv(\"cleaned_Posts_Data.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a3204-8c0e-47bf-bd7c-b81f36be1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df_posts = pd.read_csv(\"cleaned_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2d644-226f-4eb3-a4b9-d7c9730f2501",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "## Traditional Word-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7bea7-fc9e-4897-9276-97c8df0390b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level tokenizer\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7154f20-a194-4e6d-9b98-60a8fb5668ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts[\"processed\"] = df_posts[\"content_cleaned\"].apply(word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50e6cd-d4b4-412f-958a-3fb10e781501",
   "metadata": {},
   "source": [
    "## Prepare corpus for SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee33c27-4779-4cc3-8ad3-3a0a616de735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for SentencePiece\n",
    "corpus_file = \"corpus.txt\"\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in df_posts[\"content_cleaned\"]:\n",
    "        f.write(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911460ac-7497-45cb-85f1-ed1c79d2e7e9",
   "metadata": {},
   "source": [
    "### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22435aa6-f316-4096-b262-d4cd2dd6f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=bpe --vocab_size=8000 --model_type=bpe\"\n",
    ")\n",
    "bpe = spm.SentencePieceProcessor()\n",
    "bpe.load(\"bpe.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218db90-4597-4643-a8be-fd8402cf3742",
   "metadata": {},
   "source": [
    "### Unigram tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1309a67-1c85-4642-99bb-8a3d779e0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={corpus_file} --model_prefix=unigram --vocab_size=8000 --model_type=unigram\"\n",
    ")\n",
    "unigram = spm.SentencePieceProcessor()\n",
    "unigram.load(\"unigram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efc7d6-2799-49b2-aa41-f98133f45dc1",
   "metadata": {},
   "source": [
    "## Compare Tokenization Schemes Impact using Corpus Statistics\n",
    "\n",
    "Calculating:\n",
    "- vocabulary size\n",
    "- average tokens per document\n",
    "- total tokens\n",
    "- rare token frequency (<3 occurrences)\n",
    "- OOV rate (for word-based tokenizers only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc559e0-ef35-44b1-a305-7b918ad63139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus statistics per tokenizer\n",
    "\n",
    "def compute_stats(tokenizer_fn, tokenizer_name):\n",
    "    all_tokens = []\n",
    "    doc_lengths = []\n",
    "\n",
    "    for text in df_posts[\"content_cleaned\"]:\n",
    "        tokens = tokenizer_fn(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        doc_lengths.append(len(tokens))\n",
    "\n",
    "    vocab = set(all_tokens)\n",
    "    counter = collections.Counter(all_tokens)\n",
    "\n",
    "    return {\n",
    "        \"Tokenizer\": tokenizer_name,\n",
    "        \"Vocabulary Size\": len(vocab),\n",
    "        \"Total Tokens\": len(all_tokens),\n",
    "        \"Avg Tokens per Doc\": sum(doc_lengths)/len(doc_lengths),\n",
    "        \"Rare Tokens (<3)\": sum(1 for t,c in counter.items() if c < 3)\n",
    "    }\n",
    "\n",
    "stats_word = compute_stats(word_tokenizer, \"Word+Lemmatization\")\n",
    "stats_bpe = compute_stats(lambda t: bpe.encode(t, out_type=str), \"BPE\")\n",
    "stats_unigram = compute_stats(lambda t: unigram.encode(t, out_type=str), \"Unigram LM\")\n",
    "\n",
    "stats_df = pd.DataFrame([stats_word, stats_bpe, stats_unigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98091a65-06f3-4fdf-a1e1-12e30aa6d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee644717-5b74-4dfb-b83f-6920a2d0a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to Tokenizer (for easier plotting)\n",
    "plot_df = stats_df.set_index(\"Tokenizer\")\n",
    "\n",
    "# Plot each metric\n",
    "plot_df.plot(kind=\"bar\", figsize=(10,6))\n",
    "plt.title(\"Comparison of Tokenizer Statistics\")\n",
    "plt.xlabel(\"Tokenizer\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef04a8f-a35d-4f63-a1a6-ff2c18d2c02c",
   "metadata": {},
   "source": [
    "The tokenization analysis shows clear differences between the three approaches: Word+Lemmatization, BPE, and Unigram LM‚Äîand the results strongly align with theoretical expectations. The word-level tokenizer produced an extremely large vocabulary of 47,222 items and a very high number of rare tokens (31,098), even after lemmatization and stopword removal. This indicates severe sparsity: each spelling variant, slang term, and morphological form becomes a separate token, making the representation unstable and prone to poor generalization. In contrast, BPE reduced the vocabulary size to 8,887 and dropped rare tokens to just 1,023, demonstrating its ability to decompose infrequent or noisy words into reusable subword units. This leads to a more robust and consistent representation, though at the cost of slightly longer token sequences, which is expected for subword models. The Unigram LM tokenizer achieved similar vocabulary compression (8,962 tokens) but yielded the lowest number of rare tokens (855), reflecting its probabilistic approach to selecting the most efficient and informative subword units. Overall, the results validate the progression predicted by NLP literature: word-level tokenization is the least efficient, BPE offers substantial improvements, and Unigram LM provides the most balanced and linguistically consistent tokenization strategy for noisy text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44a405-97c1-4b21-9468-7cbccc5bb2ba",
   "metadata": {},
   "source": [
    "# Evaluate Tokenization Schemes and effectiveness for LLM Tasks\n",
    "\n",
    "## LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09580281-328b-4ede-ae4f-06f7b61bede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU Training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414332c-d7e4-46fd-9940-014f0662f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, token_ids, seq_len=30):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+1+self.seq_len], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646096c3-06e9-4513-9a34-86333b9b8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM LM model\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        o, _ = self.lstm(e)\n",
    "        return self.fc(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaba90d-1f53-4b0f-8f5d-31ff677eeb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_lm(model, dataloader, epochs=3, lr=0.001):\n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optim.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0044e-71b7-4782-8afd-dcfdfd9f45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "def perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item() * x.numel()\n",
    "            total_tokens += x.numel()\n",
    "    return torch.exp(torch.tensor(total_loss / total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91890ad-e79e-456e-a8d0-36ba1e2e1ae6",
   "metadata": {},
   "source": [
    "### Train LSTMs per Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe1b9-d201-42ec-955c-04b0b4e6a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "SEQ = 30\n",
    "BATCH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36eecd-7cac-4b7b-82b2-07df6650cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level Tokenizer\n",
    "word_vocab = {w:i for i,w in enumerate(set(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer)\n",
    ")))}\n",
    "word_id = lambda toks: [word_vocab[t] for t in toks if t in word_vocab]\n",
    "\n",
    "word_ids = list(itertools.chain.from_iterable(\n",
    "    df_posts[\"content_cleaned\"].apply(word_tokenizer).apply(word_id)\n",
    "))\n",
    "word_dl = DataLoader(LMDataset(word_ids, SEQ), batch_size=BATCH)\n",
    "lm_word = LSTMLM(len(word_vocab)).to(device)\n",
    "lm_word = train_lm(lm_word, word_dl)\n",
    "pp_word = perplexity(lm_word, word_dl)\n",
    "print(\"Word-level LM perplexity:\", pp_word.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d62b70-6322-4c02-8cca-f1d46c30da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE\n",
    "bpe_ids = [i for t in df_posts[\"content_cleaned\"] for i in bpe.encode(t)]\n",
    "bpe_dl = DataLoader(LMDataset(bpe_ids, SEQ), batch_size=BATCH)\n",
    "lm_bpe = LSTMLM(bpe.get_piece_size()).to(device)\n",
    "lm_bpe = train_lm(lm_bpe, bpe_dl)\n",
    "pp_bpe = perplexity(lm_bpe, bpe_dl)\n",
    "print(\"BPE LM perplexity:\", pp_bpe.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8cef4-9e18-4902-a76e-421cfd7e8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram\n",
    "uni_ids = [i for t in df_posts[\"content_cleaned\"] for i in unigram.encode(t)]\n",
    "uni_dl = DataLoader(LMDataset(uni_ids, SEQ), batch_size=BATCH)\n",
    "lm_uni = LSTMLM(unigram.get_piece_size()).to(device)\n",
    "lm_uni = train_lm(lm_uni, uni_dl) \n",
    "pp_uni = perplexity(lm_uni, uni_dl)\n",
    "print(\"Unigram LM perplexity:\", pp_uni.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062470c-1250-4db1-9436-702e6239d5fc",
   "metadata": {},
   "source": [
    "### Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ab0b5-2234-455d-80e7-a9463431edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame({\n",
    "    \"Tokenizer\": [\"Word+Lemmatization\", \"BPE\", \"Unigram\"],\n",
    "    \"Vocabulary Size\": [stats_word[\"Vocabulary Size\"], stats_bpe[\"Vocabulary Size\"], stats_unigram[\"Vocabulary Size\"]],\n",
    "    \"Total Tokens\": [stats_word[\"Total Tokens\"], stats_bpe[\"Total Tokens\"], stats_unigram[\"Total Tokens\"]],\n",
    "    \"Avg Tokens per Doc\": [stats_word[\"Avg Tokens per Doc\"], stats_bpe[\"Avg Tokens per Doc\"], stats_unigram[\"Avg Tokens per Doc\"]],\n",
    "    \"Rare Tokens (<3)\": [stats_word[\"Rare Tokens (<3)\"], stats_bpe[\"Rare Tokens (<3)\"], stats_unigram[\"Rare Tokens (<3)\"]],\n",
    "    \"Perplexity\": [pp_word, pp_bpe, pp_uni]\n",
    "})\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37271053-ba84-4e6a-b42d-c8b07aab7474",
   "metadata": {},
   "source": [
    "The LSTM results clearly demonstrate how strongly recurrent models depend on the quality of tokenization. When trained on the word-level tokenizer, the LSTM produced the highest perplexity (884), largely because this tokenizer generated an extremely large vocabulary (over 36,000 unique words) and a very high proportion of rare terms. Such sparsity makes it difficult for the LSTM to learn stable embeddings or reliably predict the next token, since many words appear too infrequently for meaningful parameter updates. In contrast, both subword tokenizers (BPE and Unigram) dramatically reduced perplexity‚Äîapproximately halving it‚Äîbecause they compress the vocabulary into a smaller, denser, and more frequent token space. This reduction in sparsity enables the LSTM to learn more generalizable patterns such as common stems, prefixes, and suffixes, improving its ability to model text sequences. BPE achieved the lowest perplexity (479), slightly outperforming Unigram, likely because BPE produces more deterministic and consistent subword units that better suit the LSTM‚Äôs sequential prediction process. Overall, the results show that subword tokenization substantially enhances LSTM performance by providing a more compact and informative input representation, while word-level tokenization overwhelms the model and leads to significantly poorer language modeling ability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GPU Env",
   "language": "python",
   "name": "gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
