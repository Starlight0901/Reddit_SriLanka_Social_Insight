{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6097bf05-6241-43f3-934c-0ca02218e8f6",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672397e2-7d58-4142-a3b2-b5dd095428d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk spacy emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ff0e2-6eae-4c89-a7f1-41e6072aa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1dd7c-fc07-4994-8853-bcad1c6ed0ac",
   "metadata": {},
   "source": [
    "# Commit to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b657452-1d16-4c01-9ed3-e38dfd5dc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd                # shows your current folder\n",
    "!git status         # check uncommitted changes\n",
    "!git add .\n",
    "!git commit -m \"Data text Preprocessing and cleaning\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260df94-6335-4131-b1c5-962708bb99d7",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141bad06-47a7-4b41-bbfd-0e62b5c7de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tharu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from langdetect import detect\n",
    "import langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601a289-471b-440f-8e73-57d9c0c0120a",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237f29ed-ddac-4873-b8f7-b54a183f4f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tharu\\AppData\\Local\\Temp\\ipykernel_14140\\3966237626.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source keyword       id   created_utc             author subreddit  \\\n",
       "0  post    new     NaN  1otaemb  1.762771e+09     Cookiehere6969  srilanka   \n",
       "1  post    new     NaN  1otaam5  1.762770e+09           oshan789  srilanka   \n",
       "2  post    new     NaN  1ot9w1v  1.762769e+09          mgssjjsks  srilanka   \n",
       "3  post    new     NaN  1ot9kwe  1.762768e+09  Critical_Rise_exe  srilanka   \n",
       "4  post    new     NaN  1ot9h2f  1.762767e+09       No-Leave8971  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                  Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "\n",
       "  parent_post  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_posts = pd.read_csv(\"Final_Posts_Data.csv\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc654212-751a-4707-ae63-580c3f5c958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (70861, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset shape:\", df_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115c4822-97df-4205-a720-40df51d9dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates based on 'content':\n",
      "Empty DataFrame\n",
      "Columns: [type, source, keyword, id, created_utc, author, subreddit, content, score, num_comments, parent_post]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on a specific column\n",
    "duplicates_name = df_posts.duplicated(subset=['content'])\n",
    "print(\"\\nDuplicates based on 'content':\")\n",
    "print(df_posts[duplicates_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8c3f2e-a6a3-4abf-9a83-44d8c0527e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971e6205-4161-488b-95aa-fe05e2398424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword         53250\n",
      "id                  0\n",
      "created_utc         0\n",
      "author           2981\n",
      "subreddit           0\n",
      "content             1\n",
      "score               8\n",
      "num_comments    52021\n",
      "parent_post     18848\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc55767c-f6b5-4d1b-a33d-1d52e211ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " type                0\n",
      "source              0\n",
      "keyword             0\n",
      "id                  0\n",
      "created_utc         0\n",
      "author              0\n",
      "subreddit           0\n",
      "content             0\n",
      "score               0\n",
      "num_comments    52020\n",
      "parent_post         0\n",
      "dtype: int64\n",
      "\n",
      " Dataset shape: (70860, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>1.762771e+09</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>1.762770e+09</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>1.762769e+09</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>1.762768e+09</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>1.762767e+09</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id   created_utc             author  \\\n",
       "0  post    new  no keyword  1otaemb  1.762771e+09     Cookiehere6969   \n",
       "1  post    new  no keyword  1otaam5  1.762770e+09           oshan789   \n",
       "2  post    new  no keyword  1ot9w1v  1.762769e+09          mgssjjsks   \n",
       "3  post    new  no keyword  1ot9kwe  1.762768e+09  Critical_Rise_exe   \n",
       "4  post    new  no keyword  1ot9h2f  1.762767e+09       No-Leave8971   \n",
       "\n",
       "  subreddit                                            content  score  \\\n",
       "0  srilanka  Is this a Scam or good investment? Haritha Lan...    2.0   \n",
       "1  srilanka  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0   \n",
       "2  srilanka  Whats your hot take on Sri Lanka as the title ...    3.0   \n",
       "3  srilanka                  Is the rs.11 deals real in Daraz?    1.0   \n",
       "4  srilanka  Need advice from the experts üôè [](https://www....    2.0   \n",
       "\n",
       "   num_comments parent_post  \n",
       "0           1.0     no post  \n",
       "1           0.0     no post  \n",
       "2           8.0     no post  \n",
       "3           3.0     no post  \n",
       "4           0.0     no post  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill 'keyword' missing values\n",
    "df_posts['keyword'] = df_posts['keyword'].fillna('no keyword')\n",
    "\n",
    "# Fill 'author' missing values\n",
    "df_posts['author'] = df_posts['author'].fillna('no author')\n",
    "\n",
    "# Fill 'score' missing values with the median\n",
    "median_score = df_posts['score'].median()\n",
    "df_posts['score'] = df_posts['score'].fillna(median_score)\n",
    "\n",
    "# Fill 'num_comments' missing values with 0 only where source == 'comments'\n",
    "mask = df_posts['source'] == 'comments'\n",
    "df_posts.loc[mask, 'num_comments'] = df_posts.loc[mask, 'num_comments'].fillna(0)\n",
    "\n",
    "# Fill 'parent_post' missing values\n",
    "df_posts['parent_post'] = df_posts['parent_post'].fillna('no post')\n",
    "\n",
    "# Drop rows where 'content' is missing (only 1 row)\n",
    "df_posts = df_posts.dropna(subset=['content'])\n",
    "# reset the index\n",
    "df_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "missing_counts = df_posts.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts) \n",
    "\n",
    "print(\"\\n Dataset shape:\", df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9bf730-1b88-45fc-8826-c2191464a088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:40:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70855</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozebo7</td>\n",
       "      <td>Dhanagg</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>NEWSWIRE\\n\\nSri Lanka flags\\noutside Rawalpind...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:55:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70856</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozdn89</td>\n",
       "      <td>Unreal_realist-7381</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>DOT STUDIOS PRESENTS\\nPASAN DOMINIC HASALAKA T...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70857</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozdi55</td>\n",
       "      <td>wiknew1</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>sarasavi fi Q\\n\\nTHE BOOKSHOP\\nOL LIST SARASAV...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>11:07:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70858</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ozbed2</td>\n",
       "      <td>smllcheeseburger</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Lamborghini Urus Twin turbo V8 2025\\n\\nPosted ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>08:54:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70859</th>\n",
       "      <td>img_post</td>\n",
       "      <td>url</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1oz8366</td>\n",
       "      <td>Crimson_roses154</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Lid ao ol on oe at jas\\nPTAA UO ¬©</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>05:28:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70860 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           type source     keyword       id               author subreddit  \\\n",
       "0          post    new  no keyword  1otaemb       Cookiehere6969  srilanka   \n",
       "1          post    new  no keyword  1otaam5             oshan789  srilanka   \n",
       "2          post    new  no keyword  1ot9w1v            mgssjjsks  srilanka   \n",
       "3          post    new  no keyword  1ot9kwe    Critical_Rise_exe  srilanka   \n",
       "4          post    new  no keyword  1ot9h2f         No-Leave8971  srilanka   \n",
       "...         ...    ...         ...      ...                  ...       ...   \n",
       "70855  img_post    url  no keyword  1ozebo7              Dhanagg  srilanka   \n",
       "70856  img_post    url  no keyword  1ozdn89  Unreal_realist-7381  srilanka   \n",
       "70857  img_post    url  no keyword  1ozdi55              wiknew1  srilanka   \n",
       "70858  img_post    url  no keyword  1ozbed2     smllcheeseburger  srilanka   \n",
       "70859  img_post    url  no keyword  1oz8366     Crimson_roses154  srilanka   \n",
       "\n",
       "                                                 content  score  num_comments  \\\n",
       "0      Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1      Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2      Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                      Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4      Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "...                                                  ...    ...           ...   \n",
       "70855  NEWSWIRE\\n\\nSri Lanka flags\\noutside Rawalpind...    3.0           NaN   \n",
       "70856  DOT STUDIOS PRESENTS\\nPASAN DOMINIC HASALAKA T...    3.0           NaN   \n",
       "70857  sarasavi fi Q\\n\\nTHE BOOKSHOP\\nOL LIST SARASAV...    3.0           NaN   \n",
       "70858  Lamborghini Urus Twin turbo V8 2025\\n\\nPosted ...    3.0           NaN   \n",
       "70859                  Lid ao ol on oe at jas\\nPTAA UO ¬©    3.0           NaN   \n",
       "\n",
       "      parent_post created_date created_time  \n",
       "0         no post   2025-11-10     10:33:16  \n",
       "1         no post   2025-11-10     10:26:02  \n",
       "2         no post   2025-11-10     10:00:29  \n",
       "3         no post   2025-11-10     09:40:57  \n",
       "4         no post   2025-11-10     09:33:57  \n",
       "...           ...          ...          ...  \n",
       "70855     no post   2025-11-17     11:55:19  \n",
       "70856     no post   2025-11-17     11:16:00  \n",
       "70857     no post   2025-11-17     11:07:48  \n",
       "70858     no post   2025-11-17     08:54:42  \n",
       "70859     no post   2025-11-17     05:28:02  \n",
       "\n",
       "[70860 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting date column into a readable format\n",
    "df_posts['created_date'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.date\n",
    "df_posts['created_time'] = pd.to_datetime(df_posts['created_utc'], unit='s').dt.time\n",
    "\n",
    "# Drop the original 'created_utc' column\n",
    "df_posts.drop(columns=['created_utc'], inplace=True)\n",
    "df_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b4e95-d80e-45a3-a350-8c9920010d27",
   "metadata": {},
   "source": [
    "### Drop non-english data (sinhala and tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8f705a-fb07-4324-83f1-f3198e3ebb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.count of 0        en\n",
       "1        en\n",
       "2        en\n",
       "3        en\n",
       "4        en\n",
       "         ..\n",
       "70855    id\n",
       "70856    en\n",
       "70857    en\n",
       "70858    en\n",
       "70859    et\n",
       "Name: language, Length: 70860, dtype: object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Create a new column for language\n",
    "df_posts['language'] = df_posts['content'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f0387c0-c623-44f6-bd16-9e0c725d21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69773, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>parent_post</th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaemb</td>\n",
       "      <td>Cookiehere6969</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is this a Scam or good investment? Haritha Lan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:33:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1otaam5</td>\n",
       "      <td>oshan789</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Villa units for sale in Unawatuna Sri Lanka ! ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:26:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9w1v</td>\n",
       "      <td>mgssjjsks</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Whats your hot take on Sri Lanka as the title ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>10:00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9kwe</td>\n",
       "      <td>Critical_Rise_exe</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Is the rs.11 deals real in Daraz?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:40:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>new</td>\n",
       "      <td>no keyword</td>\n",
       "      <td>1ot9h2f</td>\n",
       "      <td>No-Leave8971</td>\n",
       "      <td>srilanka</td>\n",
       "      <td>Need advice from the experts üôè [](https://www....</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no post</td>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>09:33:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type source     keyword       id             author subreddit  \\\n",
       "0  post    new  no keyword  1otaemb     Cookiehere6969  srilanka   \n",
       "1  post    new  no keyword  1otaam5           oshan789  srilanka   \n",
       "2  post    new  no keyword  1ot9w1v          mgssjjsks  srilanka   \n",
       "3  post    new  no keyword  1ot9kwe  Critical_Rise_exe  srilanka   \n",
       "4  post    new  no keyword  1ot9h2f       No-Leave8971  srilanka   \n",
       "\n",
       "                                             content  score  num_comments  \\\n",
       "0  Is this a Scam or good investment? Haritha Lan...    2.0           1.0   \n",
       "1  Villa units for sale in Unawatuna Sri Lanka ! ...    3.0           0.0   \n",
       "2  Whats your hot take on Sri Lanka as the title ...    3.0           8.0   \n",
       "3                  Is the rs.11 deals real in Daraz?    1.0           3.0   \n",
       "4  Need advice from the experts üôè [](https://www....    2.0           0.0   \n",
       "\n",
       "  parent_post created_date created_time  \n",
       "0     no post   2025-11-10     10:33:16  \n",
       "1     no post   2025-11-10     10:26:02  \n",
       "2     no post   2025-11-10     10:00:29  \n",
       "3     no post   2025-11-10     09:40:57  \n",
       "4     no post   2025-11-10     09:33:57  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only English posts\n",
    "df_posts = df_posts[df_posts['language'] == 'en'].copy()\n",
    "\n",
    "# Drop the language column\n",
    "df_posts.drop(columns=['language'], inplace=True)\n",
    "print(df_posts.shape)\n",
    "\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2868e0ae-d838-4d46-a2ae-be17eca49662",
   "metadata": {},
   "source": [
    "### **1. Language Filtering**\n",
    "\n",
    "* [‚úîÔ∏è] Detect language of each post.\n",
    "* [‚úîÔ∏è] Remove posts in Sinhala, Tamil, or any non-English languages.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Text Normalization**\n",
    "\n",
    "* [ ] Convert all text to lowercase.\n",
    "* [ ] Strip leading and trailing whitespaces.\n",
    "* [ ] Normalize unicode characters (optional, e.g., accented letters).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Removing Irrelevant Content**\n",
    "\n",
    "* [ ] Remove URLs (e.g., `http://`, `https://`, `www.`).\n",
    "* [ ] Remove emojis and emoticons.\n",
    "* [ ] Remove platform-specific metadata (e.g., `reddit`, `u/username`, `r/subreddit`, `comments`).\n",
    "* [ ] Remove escape sequences (e.g., `\\n`, `\\t`).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Handling Special Characters**\n",
    "\n",
    "* [ ] Remove punctuation marks (`!`, `.`, `,`, `?`, etc.).\n",
    "* [ ] Remove other non-alphanumeric characters (`@`, `#`, `%`, `^`, etc.).\n",
    "* [ ] Optionally remove numbers (if not needed for analysis).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Tokenization and Text Structuring**\n",
    "\n",
    "* [ ] Tokenize text into words (if needed for further analysis).\n",
    "* [ ] Optionally remove stopwords (`the`, `is`, `and`, etc.).\n",
    "* [ ] Optionally lemmatize or stem words.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Filtering by Length**\n",
    "\n",
    "* [ ] Remove posts with fewer than 25 words.\n",
    "* [ ] Remove posts with more than 1000 words.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Corpus Analysis**\n",
    "\n",
    "* [ ] Calculate the total number of words in the cleaned dataset.\n",
    "* [ ] Calculate the number of unique words in the cleaned dataset.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Optional / Advanced Steps**\n",
    "\n",
    "* [ ] Handle duplicate posts if any.\n",
    "* [ ] Correct common typos or spelling errors.\n",
    "* [ ] Normalize spacing between words.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also make a **ready-to-use Python pipeline** that implements **all these steps in one go** for your `content` column and reports corpus size and unique words automatically.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65b95f-5a8c-4d3f-9bdf-7bda2e0014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install nltk spacy emoji\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Load English model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to replace emojis with textual description\n",
    "def emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # 1. Convert emojis to text\n",
    "    text = emoji_to_text(text)\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove Reddit platform metadata (u/username, r/subreddit)\n",
    "    text = re.sub(r'u\\/\\w+', '', text)\n",
    "    text = re.sub(r'r\\/\\w+', '', text)\n",
    "    \n",
    "    # 4. Remove escape sequences and extra whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 5. Remove punctuation (except within words like can't, won't)\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    \n",
    "    # 6. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 7. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 8. Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # 9. Lemmatization\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # 10. Join back into string\n",
    "    cleaned_text = \" \".join(lemmatized)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning function\n",
    "df_posts['content_cleaned'] = df_posts['content'].astype(str).apply(clean_text)\n",
    "\n",
    "# 11. Filter posts by word count\n",
    "min_words = 25\n",
    "max_words = 1000\n",
    "\n",
    "def word_count_filter(text):\n",
    "    count = len(text.split())\n",
    "    return min_words <= count <= max_words\n",
    "\n",
    "df_posts = df_posts[df_posts['content_cleaned'].apply(word_count_filter)]\n",
    "\n",
    "# 12. Report corpus statistics\n",
    "all_text = \" \".join(df_posts['content_cleaned'])\n",
    "all_words = all_text.split()\n",
    "total_words = len(all_words)\n",
    "unique_words = len(set(all_words))\n",
    "\n",
    "print(f\"Total words in cleaned corpus: {total_words}\")\n",
    "print(f\"Unique words in cleaned corpus: {unique_words}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
